---
phase: 05-llm-judge-evaluation
plan: 02
type: execute
wave: 2
depends_on: [05-01]
files_modified:
  - src/salvo/evaluation/evaluators/base.py
  - src/salvo/evaluation/scorer.py
  - src/salvo/execution/trial_runner.py
  - src/salvo/models/result.py
  - src/salvo/models/trial.py
  - src/salvo/cli/output.py
  - src/salvo/cli/run_cmd.py
  - tests/test_judge_integration.py
  - tests/test_evaluation_scorer.py
autonomous: true
requirements: [JUDG-01, JUDG-02]

must_haves:
  truths:
    - "Judge assertions are evaluated asynchronously within the existing async trial runner pipeline without event loop conflicts"
    - "Judge cost is tracked separately from agent cost and both are visible in CLI output"
    - "Active judge model and k value are displayed in CLI output and stored in run metadata"
    - "Individual judge verdicts (per-criterion scores across k votes) are visible in detail output"
    - "A scenario with mixed standard + judge assertions produces correct weighted scores"
  artifacts:
    - path: "src/salvo/evaluation/evaluators/base.py"
      provides: "BaseEvaluator with async evaluate_async default method"
      contains: "async def evaluate_async"
    - path: "src/salvo/evaluation/scorer.py"
      provides: "Async evaluation pipeline via evaluate_trace_async"
      contains: "async def evaluate_trace_async"
    - path: "src/salvo/execution/trial_runner.py"
      provides: "Trial runner calling evaluate_trace_async for judge-aware evaluation"
      contains: "evaluate_trace_async"
    - path: "src/salvo/models/result.py"
      provides: "EvalResult with optional metadata dict for structured judge data"
      contains: "metadata"
    - path: "src/salvo/models/trial.py"
      provides: "TrialSuiteResult with judge_cost_total field"
      contains: "judge_cost_total"
    - path: "src/salvo/cli/output.py"
      provides: "CLI output showing judge model, k, and cost breakdown"
      contains: "judge"
  key_links:
    - from: "src/salvo/execution/trial_runner.py"
      to: "src/salvo/evaluation/scorer.py"
      via: "Trial runner calls evaluate_trace_async instead of evaluate_trace"
      pattern: "evaluate_trace_async"
    - from: "src/salvo/evaluation/scorer.py"
      to: "src/salvo/evaluation/evaluators/base.py"
      via: "evaluate_trace_async awaits evaluator.evaluate_async() for each assertion"
      pattern: "await.*evaluate_async"
    - from: "src/salvo/cli/output.py"
      to: "src/salvo/models/trial.py"
      via: "render_headline reads judge_cost_total for cost breakdown display"
      pattern: "judge_cost_total"
---

<objective>
Wire the JudgeEvaluator (built in 05-01) into the async evaluation pipeline, trial runner, and CLI output. Add async evaluation path so judge assertions can make LLM calls without event loop conflicts. Track and display judge cost separately from agent cost. Show judge model, k, and per-criterion details in CLI output.

Purpose: Without this integration, the judge evaluator exists but cannot be invoked from the normal `salvo run` pipeline. This plan makes judge assertions work end-to-end: from scenario YAML through evaluation to terminal output.
Output: Working end-to-end judge pipeline visible in `salvo run` output with cost breakdown and per-criterion details.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-llm-judge-evaluation/05-RESEARCH.md
@.planning/phases/05-llm-judge-evaluation/05-01-SUMMARY.md
@src/salvo/evaluation/evaluators/base.py
@src/salvo/evaluation/evaluators/__init__.py
@src/salvo/evaluation/scorer.py
@src/salvo/execution/trial_runner.py
@src/salvo/models/result.py
@src/salvo/models/trial.py
@src/salvo/cli/output.py
@src/salvo/cli/run_cmd.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Async evaluation pipeline and trial runner integration</name>
  <files>
    src/salvo/evaluation/evaluators/base.py
    src/salvo/evaluation/scorer.py
    src/salvo/execution/trial_runner.py
    src/salvo/models/result.py
    src/salvo/models/trial.py
    tests/test_evaluation_scorer.py
    tests/test_judge_integration.py
  </files>
  <action>
    **Extend BaseEvaluator (src/salvo/evaluation/evaluators/base.py):**

    Add a default `evaluate_async` method that delegates to the sync `evaluate()`:
    ```python
    async def evaluate_async(self, trace: RunTrace, assertion: dict) -> EvalResult:
        """Async evaluation. Default delegates to sync evaluate().

        Subclasses that need async (e.g., JudgeEvaluator) override this
        method with their async implementation.
        """
        return self.evaluate(trace, assertion)
    ```
    This is NOT abstract -- it has a concrete default. Existing evaluators (JMESPath, tool_sequence, cost_limit, latency_limit) inherit this default and work unchanged. Only JudgeEvaluator overrides it.

    **Extend EvalResult (src/salvo/models/result.py):**

    Add an optional `metadata` field to EvalResult for structured judge data:
    ```python
    metadata: dict | None = None
    ```
    This field stores structured data like `{"judge_model": "gpt-4o-mini", "judge_k": 3, "judge_cost_usd": 0.001234, "per_criterion": [...]}`. The existing `details` string remains for human-readable output. The `metadata` field enables programmatic access to judge cost for aggregation.

    IMPORTANT: EvalResult does NOT have `extra='forbid'` currently -- it's a plain BaseModel. Adding the field is safe without changing model_config.

    **Add evaluate_trace_async (src/salvo/evaluation/scorer.py):**

    Add a new async function alongside the existing sync `evaluate_trace`:

    ```python
    async def evaluate_trace_async(
        trace: RunTrace,
        assertions: list[dict],
        threshold: float,
    ) -> tuple[list[EvalResult], float, bool]:
        """Async evaluation orchestration -- supports async evaluators.

        Same contract as evaluate_trace but awaits evaluate_async() on
        each evaluator, enabling JudgeEvaluator to make LLM calls.
        """
        eval_results: list[EvalResult] = []
        for assertion in assertions:
            evaluator = get_evaluator(assertion["type"])
            result = await evaluator.evaluate_async(trace, assertion)
            eval_results.append(result)
        score, passed, _ = compute_score(eval_results, threshold)
        return (eval_results, score, passed)
    ```

    The sync `evaluate_trace` remains unchanged for backward compatibility (tests, CLI validate, etc).

    **Update trial_runner.py:**

    In `_execute_single_trial`, replace the sync `evaluate_trace` call with `evaluate_trace_async`:

    Change:
    ```python
    from salvo.evaluation.scorer import evaluate_trace
    ...
    eval_results, score, passed = evaluate_trace(
        trace, raw_assertions, self._threshold,
    )
    ```
    To:
    ```python
    from salvo.evaluation.scorer import evaluate_trace_async
    ...
    eval_results, score, passed = await evaluate_trace_async(
        trace, raw_assertions, self._threshold,
    )
    ```

    Also, before calling evaluate_trace_async, inject the scenario reference into judge-type assertions so the JudgeEvaluator can access system_prompt and tools for context building:
    ```python
    for a in raw_assertions:
        if a.get("type") == "judge":
            a["_scenario"] = self._scenario
    ```

    **Extend TrialSuiteResult (src/salvo/models/trial.py):**

    Add optional judge cost tracking field:
    ```python
    judge_cost_total: float | None = None
    ```

    **Update _build_suite_result in trial_runner.py:**

    After building the suite result, aggregate judge costs from EvalResult metadata across all trials:
    ```python
    # Aggregate judge costs from eval result metadata
    judge_cost = 0.0
    has_judge = False
    for trial in results:
        for er in trial.eval_results:
            if er.metadata and "judge_cost_usd" in er.metadata:
                judge_cost += er.metadata["judge_cost_usd"]
                has_judge = True
    ```
    Set `judge_cost_total=judge_cost if has_judge else None` in the TrialSuiteResult.

    **Update tests:**

    `tests/test_evaluation_scorer.py` -- add:
    - `test_evaluate_trace_async_same_as_sync` -- verify async produces same results as sync for standard evaluators
    - `test_evaluate_trace_async_with_mock_judge` -- mock a judge evaluator returning known EvalResult, verify it's included in results

    `tests/test_judge_integration.py` -- end-to-end integration tests:
    - `test_e2e_judge_assertion_in_trial` -- create a Scenario with a judge assertion, mock adapter for both agent AND judge calls, run through TrialRunner, verify judge assertion appears in eval_results
    - `test_e2e_mixed_assertions` -- scenario with JMESPath + judge assertions, verify both evaluated and weighted correctly
    - `test_e2e_judge_cost_tracked_in_suite` -- verify judge_cost_total populated in TrialSuiteResult

    For integration tests, use two mock adapters: one for agent execution (via adapter_factory) and one for judge calls (patch get_adapter inside the judge evaluator). The agent mock returns a simple response. The judge mock returns a tool call with valid scores.
  </action>
  <verify>
    cd /Users/apple/JHERLETH/agent-something && source .venv/bin/activate && python -m pytest tests/test_evaluation_scorer.py tests/test_judge_integration.py -v
  </verify>
  <done>
    BaseEvaluator has evaluate_async default delegating to sync. evaluate_trace_async orchestrates async evaluation. Trial runner uses evaluate_trace_async and injects _scenario for judge context. EvalResult has metadata dict for structured judge data. TrialSuiteResult has judge_cost_total. Judge costs aggregated from eval result metadata across trials. All tests pass including existing scorer tests (no regressions).
  </done>
</task>

<task type="auto">
  <name>Task 2: CLI output for judge model, cost breakdown, and per-criterion details</name>
  <files>
    src/salvo/cli/output.py
    src/salvo/cli/run_cmd.py
  </files>
  <action>
    **Update render_headline in output.py:**

    Modify the Cost row to show agent + judge breakdown when judge costs are present:

    Current:
    ```python
    if suite.cost_total is not None and suite.cost_avg_per_trial is not None:
        table.add_row(
            "Cost",
            f"total=${suite.cost_total:.4f} avg=${suite.cost_avg_per_trial:.4f}/trial",
        )
    ```

    Updated: If `suite.judge_cost_total` is not None and > 0, show breakdown:
    ```python
    if suite.cost_total is not None and suite.cost_avg_per_trial is not None:
        cost_str = f"total=${suite.cost_total:.4f} avg=${suite.cost_avg_per_trial:.4f}/trial"
        if suite.judge_cost_total is not None and suite.judge_cost_total > 0:
            agent_cost = suite.cost_total - suite.judge_cost_total
            cost_str = (
                f"total=${suite.cost_total:.4f} "
                f"(agent=${agent_cost:.4f} + judge=${suite.judge_cost_total:.4f}) "
                f"avg=${suite.cost_avg_per_trial:.4f}/trial"
            )
        table.add_row("Cost", cost_str)
    ```

    Add a new "Judge" row after the Model row (if judge assertions detected). Detect by checking if any trial's eval_results have assertion_type="judge" with metadata containing judge_model and judge_k:
    ```python
    # Find judge info from first trial's judge eval result metadata
    judge_info = None
    for trial in suite.trials:
        for er in trial.eval_results:
            if er.assertion_type == "judge" and er.metadata:
                judge_info = er.metadata
                break
        if judge_info:
            break

    if judge_info:
        jm = judge_info.get("judge_model", "unknown")
        jk = judge_info.get("judge_k", "?")
        table.add_row("Judge", f"model={jm} k={jk}")
    ```

    **Update render_details in output.py:**

    Add a "Judge Criteria" section that shows per-criterion median scores when judge assertions are present. Find all judge eval results across trials and collect their per-criterion details from metadata:

    After the "Score breakdown" section and before "Sample Failures":
    ```python
    # Judge criteria breakdown
    judge_criteria = []
    for trial in suite.trials:
        for er in trial.eval_results:
            if er.assertion_type == "judge" and er.metadata and "per_criterion" in er.metadata:
                judge_criteria = er.metadata["per_criterion"]
                break
        if judge_criteria:
            break

    if judge_criteria:
        console.print("[bold]Judge Criteria[/bold]")
        for pc in judge_criteria:
            name = pc.get("name", "unknown")
            median = pc.get("median_score", 0.0)
            weight = pc.get("weight", 1.0)
            all_scores = pc.get("all_scores", [])
            scores_str = ", ".join(f"{s:.2f}" for s in all_scores)
            # Color-code median: green >= 0.7, yellow >= 0.4, red < 0.4
            if median >= 0.7:
                style = "green"
            elif median >= 0.4:
                style = "yellow"
            else:
                style = "red"
            console.print(
                f"  [{style}]{name}[/{style}]: median={median:.2f} "
                f"scores=[{scores_str}] weight={weight:.1f}"
            )
        console.print()
    ```

    **Update render_details cost breakdown:**

    When judge cost is present, update the Cost section to show the breakdown:
    ```python
    if suite.cost_total is not None:
        costs = [t.cost_usd for t in suite.trials if t.cost_usd is not None]
        if costs:
            cost_str = (
                f"total=${suite.cost_total:.4f} "
                f"min=${min(costs):.4f} max=${max(costs):.4f} "
                f"avg=${suite.cost_avg_per_trial:.4f}/trial"
            )
            if suite.judge_cost_total is not None and suite.judge_cost_total > 0:
                agent_cost = suite.cost_total - suite.judge_cost_total
                cost_str += f" (agent=${agent_cost:.4f} + judge=${suite.judge_cost_total:.4f})"
            console.print(f"[bold]Cost:[/bold] {cost_str}")
            console.print()
    ```

    **No changes needed to run_cmd.py** -- the run command already uses TrialRunner which now calls evaluate_trace_async internally. The adapter_factory and scenario flow are unchanged. The only update: ensure the cost_total in the suite result includes judge cost. This happens naturally because TrialRunner._build_suite_result now adds judge_cost_total. However, the `cost_total` in TrialSuiteResult is computed from trial-level `cost_usd` fields which only capture agent cost. The judge cost is additional.

    WAIT -- important correction. The existing `cost_total` is computed from `trial.cost_usd` (agent cost per trial). Judge cost is NOT included in `trial.cost_usd` -- it comes from EvalResult metadata. So `suite.cost_total` is agent-only cost. The total combined cost = `suite.cost_total + suite.judge_cost_total`. Update the CLI cost display:

    In render_headline:
    ```python
    if suite.cost_total is not None and suite.cost_avg_per_trial is not None:
        agent_cost = suite.cost_total
        judge_cost = suite.judge_cost_total or 0.0
        combined = agent_cost + judge_cost
        if judge_cost > 0:
            cost_str = (
                f"total=${combined:.4f} "
                f"(agent=${agent_cost:.4f} + judge=${judge_cost:.4f}) "
                f"avg=${suite.cost_avg_per_trial:.4f}/trial"
            )
        else:
            cost_str = f"total=${agent_cost:.4f} avg=${suite.cost_avg_per_trial:.4f}/trial"
        table.add_row("Cost", cost_str)
    ```

    Same correction in render_details cost section.

    No new test file needed for output.py -- existing rendering tests (if any) will cover regressions. The judge_integration tests from Task 1 verify the data flow.
  </action>
  <verify>
    cd /Users/apple/JHERLETH/agent-something && source .venv/bin/activate && python -m pytest tests/ -v --tb=short 2>&1 | tail -30
  </verify>
  <done>
    CLI headline shows "Judge" row with model and k when judge assertions present. Cost row shows combined total with agent/judge breakdown. Detail sections include "Judge Criteria" with per-criterion median scores and k-vote score distribution. All existing tests pass with no regressions.
  </done>
</task>

</tasks>

<verification>
```bash
cd /Users/apple/JHERLETH/agent-something && source .venv/bin/activate

# All tests pass
python -m pytest tests/ -v --tb=short

# Verify async evaluation path works
python -c "
import asyncio
from salvo.evaluation.scorer import evaluate_trace_async
print('evaluate_trace_async importable')
"

# Verify trial runner uses async evaluation
python -c "
import inspect
from salvo.execution.trial_runner import TrialRunner
source = inspect.getsource(TrialRunner._execute_single_trial)
assert 'evaluate_trace_async' in source, 'Trial runner should use evaluate_trace_async'
print('Trial runner uses evaluate_trace_async')
"

# Verify judge cost field exists
python -c "
from salvo.models.trial import TrialSuiteResult
fields = TrialSuiteResult.model_fields
assert 'judge_cost_total' in fields, 'judge_cost_total field missing'
print(f'judge_cost_total field type: {fields[\"judge_cost_total\"].annotation}')
"

# Verify EvalResult metadata field
python -c "
from salvo.models.result import EvalResult
fields = EvalResult.model_fields
assert 'metadata' in fields, 'metadata field missing'
print(f'metadata field type: {fields[\"metadata\"].annotation}')
"
```
</verification>

<success_criteria>
- BaseEvaluator has evaluate_async default that delegates to sync evaluate
- evaluate_trace_async awaits evaluator.evaluate_async for each assertion
- Trial runner uses evaluate_trace_async -- no event loop conflicts for judge evaluator
- EvalResult has optional metadata dict for structured judge data
- TrialSuiteResult has judge_cost_total for separate judge cost tracking
- Judge costs aggregated from eval result metadata across all trials
- CLI headline shows Judge row with model and k
- CLI Cost row shows combined total with agent/judge breakdown when judge assertions present
- CLI detail section shows Judge Criteria with per-criterion scores and k-vote distribution
- All existing tests pass with zero regressions
</success_criteria>

<output>
After completion, create `.planning/phases/05-llm-judge-evaluation/05-02-SUMMARY.md`
</output>
