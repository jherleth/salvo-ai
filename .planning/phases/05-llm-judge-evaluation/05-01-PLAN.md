---
phase: 05-llm-judge-evaluation
plan: 01
type: tdd
wave: 1
depends_on: []
files_modified:
  - src/salvo/models/config.py
  - src/salvo/models/scenario.py
  - src/salvo/evaluation/judge/__init__.py
  - src/salvo/evaluation/judge/prompt.py
  - src/salvo/evaluation/judge/context.py
  - src/salvo/evaluation/judge/extraction.py
  - src/salvo/evaluation/judge/aggregation.py
  - src/salvo/evaluation/evaluators/judge.py
  - src/salvo/evaluation/evaluators/__init__.py
  - tests/test_judge_prompt.py
  - tests/test_judge_context.py
  - tests/test_judge_extraction.py
  - tests/test_judge_aggregation.py
  - tests/test_judge_evaluator.py
autonomous: true
requirements: [JUDG-01, JUDG-02]

must_haves:
  truths:
    - "A judge assertion in scenario YAML with named criteria (name/description/weight) is accepted by validation"
    - "JudgeEvaluator registered as 'judge' in EVALUATOR_REGISTRY and instantiable via get_evaluator('judge')"
    - "Judge prompt auto-generated from criteria list with 0.0-1.0 scoring scale and per-criterion evaluation instructions"
    - "Context builder produces final response + tool-call summary by default, with opt-in system prompt inclusion"
    - "Structured output extraction parses tool_call arguments into per-criterion scores, with text-JSON fallback"
    - "k-vote aggregation computes median per-criterion scores and majority-vote pass/fail"
    - "Judge config in salvo.yaml with adapter/model/k/temperature defaults, overridable per assertion"
  artifacts:
    - path: "src/salvo/models/config.py"
      provides: "JudgeConfig model and ProjectConfig.judge field"
      contains: "class JudgeConfig"
    - path: "src/salvo/models/scenario.py"
      provides: "Extended Assertion model with judge-specific optional fields"
      contains: "criteria"
    - path: "src/salvo/evaluation/judge/prompt.py"
      provides: "Judge prompt template builder from criteria"
      contains: "def build_judge_prompt"
    - path: "src/salvo/evaluation/judge/context.py"
      provides: "Trace context builder for judge evaluation"
      contains: "def build_context"
    - path: "src/salvo/evaluation/judge/extraction.py"
      provides: "Structured output extraction with text-JSON fallback"
      contains: "def extract_scores"
    - path: "src/salvo/evaluation/judge/aggregation.py"
      provides: "k-vote aggregation with median scores and majority verdict"
      contains: "def aggregate_k_votes"
    - path: "src/salvo/evaluation/evaluators/judge.py"
      provides: "JudgeEvaluator with async evaluate_async and sync evaluate"
      contains: "class JudgeEvaluator"
    - path: "src/salvo/evaluation/evaluators/__init__.py"
      provides: "Updated registry with judge entry"
      contains: "\"judge\": JudgeEvaluator"
  key_links:
    - from: "src/salvo/evaluation/evaluators/judge.py"
      to: "src/salvo/evaluation/judge/prompt.py"
      via: "JudgeEvaluator calls build_judge_prompt to generate system prompt from criteria"
      pattern: "build_judge_prompt"
    - from: "src/salvo/evaluation/evaluators/judge.py"
      to: "src/salvo/evaluation/judge/context.py"
      via: "JudgeEvaluator calls build_context to build the judge's view of the trace"
      pattern: "build_context"
    - from: "src/salvo/evaluation/evaluators/judge.py"
      to: "src/salvo/evaluation/judge/extraction.py"
      via: "JudgeEvaluator calls extract_scores on adapter result to get per-criterion scores"
      pattern: "extract_scores"
    - from: "src/salvo/evaluation/evaluators/judge.py"
      to: "src/salvo/evaluation/judge/aggregation.py"
      via: "JudgeEvaluator calls aggregate_k_votes on collected k results"
      pattern: "aggregate_k_votes"
    - from: "src/salvo/evaluation/evaluators/judge.py"
      to: "src/salvo/adapters/registry.py"
      via: "JudgeEvaluator calls get_adapter to create fresh adapter for judge calls"
      pattern: "get_adapter"
---

<objective>
Build the complete LLM judge evaluator: config models for judge settings in salvo.yaml and scenario YAML, judge prompt template builder, trace context builder, structured output extraction with text-JSON fallback, k-vote aggregation with median scores and majority verdict, and the JudgeEvaluator class that orchestrates the full judge pipeline.

Purpose: This is the core evaluation logic for Phase 5. It adds a new "judge" assertion type that enables rubric-based LLM evaluation with per-criterion scoring and k-vote reliability. All judge-specific modules live in a `evaluation/judge/` subpackage, keeping them isolated from existing evaluators.
Output: JudgeEvaluator class, 4 judge submodules, extended config/assertion models, and comprehensive TDD tests.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-llm-judge-evaluation/05-RESEARCH.md
@src/salvo/evaluation/evaluators/base.py
@src/salvo/evaluation/evaluators/__init__.py
@src/salvo/evaluation/evaluators/jmespath_eval.py
@src/salvo/evaluation/scorer.py
@src/salvo/models/config.py
@src/salvo/models/scenario.py
@src/salvo/models/result.py
@src/salvo/execution/trace.py
@src/salvo/execution/cost.py
@src/salvo/adapters/base.py
@src/salvo/adapters/registry.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Judge config models, prompt builder, context builder, and extraction</name>
  <files>
    src/salvo/models/config.py
    src/salvo/models/scenario.py
    src/salvo/evaluation/judge/__init__.py
    src/salvo/evaluation/judge/prompt.py
    src/salvo/evaluation/judge/context.py
    src/salvo/evaluation/judge/extraction.py
    tests/test_judge_prompt.py
    tests/test_judge_context.py
    tests/test_judge_extraction.py
  </files>
  <action>
    **Config models:**

    1. Extend `src/salvo/models/config.py` -- add a `JudgeConfig` Pydantic model:
       ```python
       class JudgeConfig(BaseModel):
           model_config = {"extra": "forbid"}
           adapter: str = "openai"
           model: str = "gpt-4o-mini"
           k: int = Field(default=3, ge=1, le=21)
           temperature: float = 0.0
           max_tokens: int = 1024
       ```
       Add `judge: JudgeConfig = Field(default_factory=JudgeConfig)` to `ProjectConfig`.

    2. Extend `src/salvo/models/scenario.py` `Assertion` model -- add optional judge-specific fields:
       - `criteria: list[dict] | None = None` -- list of `{name: str, description: str, weight: float}` dicts
       - `model: str | None = None` -- per-assertion judge model override (already named `model` would conflict with Pydantic... use field name `judge_model`)
       - `k: int | None = None` -- per-assertion k override
       - `include_system_prompt: bool = False` -- opt-in flag for extended context
       - `custom_prompt: str | None = None` -- fully custom judge system prompt

       IMPORTANT: The Assertion model uses `extra='forbid'`, so these fields MUST be added explicitly as optional fields on the model. Do NOT try to sneak them through extras.

       NOTE on field naming: Use `judge_model` (not `model`) to avoid shadowing Pydantic's internal `model_*` namespace. In the YAML, users will write `judge_model: gpt-4o` for per-assertion override.

    **Judge prompt builder (src/salvo/evaluation/judge/prompt.py):**

    Create `__init__.py` (empty) and `prompt.py`:

    1. `JUDGE_SYSTEM_TEMPLATE` -- a system prompt template string instructing the judge to evaluate against named criteria on a 0.0-1.0 scale. Include 5-point anchoring (0.0 = completely fails, 0.25 = mostly fails, 0.5 = partially meets, 0.75 = mostly meets, 1.0 = fully meets). Include instruction to evaluate each criterion independently. Contains `{criteria_block}` placeholder.

    2. `JUDGE_USER_TEMPLATE` -- user message template with `{context_block}` placeholder and instruction to use the score_criteria tool.

    3. `def build_criteria_block(criteria: list[dict]) -> str` -- formats each criterion as `- **{name}** (weight: {weight}): {description}` on separate lines.

    4. `def build_judge_prompt(criteria: list[dict]) -> str` -- builds the complete system prompt by rendering JUDGE_SYSTEM_TEMPLATE with the criteria block. Returns the system prompt string.

    5. `def build_scoring_tool(criteria: list[dict]) -> dict` -- builds the tool definition dict for structured score extraction. Tool name: `score_criteria`. For each criterion, create a nested object property with `score` (number, 0.0-1.0) and `reasoning` (string) sub-properties, both required. Return in the format expected by `BaseAdapter.send_turn()` tools parameter: `{"name": "score_criteria", "description": "...", "parameters": {"type": "object", "properties": {...}, "required": [...]}}`.

    6. `def format_tool_choice(provider_name: str, tool_name: str) -> dict` -- returns the provider-specific tool_choice dict. For OpenAI-like adapters (provider contains "openai"): `{"tool_choice": {"type": "function", "function": {"name": tool_name}}}`. For Anthropic-like (provider contains "anthropic"): `{"tool_choice": {"type": "tool", "name": tool_name}}`. For unknown: return empty dict (auto tool choice).

    **Context builder (src/salvo/evaluation/judge/context.py):**

    1. `def build_tool_call_summary(trace: RunTrace, max_arg_length: int = 100) -> str` -- build concise tool-call summary. For each tool call in `trace.tool_calls_made`, format as `{i}. {name}({truncated_args})`. If no tool calls, return `"No tool calls were made."`.

    2. `def build_context(trace: RunTrace, scenario: Scenario | None = None, include_system_prompt: bool = False) -> str` -- build the context block the judge sees. Always include: `## Agent's Final Response` section with `trace.final_content` (or "(empty)"), and `## Tool Calls Made` section with tool call summary. If `include_system_prompt=True` and scenario provided: add `## Scenario System Prompt` (truncated to 2000 chars) and `## Available Tools` (name + description only). Join sections with `\n\n`.

    **Structured extraction (src/salvo/evaluation/judge/extraction.py):**

    1. `def extract_scores_from_tool_call(tool_calls: list, criteria: list[dict]) -> dict | None` -- extract per-criterion scores from AdapterTurnResult.tool_calls. Look for tool call named `score_criteria`. Parse its arguments dict, which should map criterion names to `{score, reasoning}` dicts. Clamp each score to [0.0, 1.0]. Return the parsed dict or None if no matching tool call found.

    2. `def extract_json_from_text(text: str) -> dict | None` -- fallback: extract JSON from text response using three strategies: (a) direct json.loads, (b) brace extraction (first `{` to last `}`), (c) markdown code block (```json...```). Return None if all fail.

    3. `def extract_scores(result: AdapterTurnResult, criteria: list[dict]) -> dict | None` -- main entry point. Try tool_call extraction first. If None, try text-JSON fallback on `result.content`. If both fail, return None. For any extracted dict, validate that it contains at least one expected criterion name with a numeric score.

    **Tests (TDD -- write tests first, then implement):**

    `tests/test_judge_prompt.py`:
    - `test_build_criteria_block_formats_all_criteria` -- 3 criteria with different weights, verify output contains all names and weights
    - `test_build_judge_prompt_contains_scale` -- verify 0.0-1.0 scale description present
    - `test_build_judge_prompt_contains_criteria` -- verify all criteria names in output
    - `test_build_scoring_tool_schema` -- verify tool name, required properties match criteria names, each has score+reasoning
    - `test_format_tool_choice_openai` -- verify OpenAI format with function name
    - `test_format_tool_choice_anthropic` -- verify Anthropic format with tool name
    - `test_format_tool_choice_unknown` -- returns empty dict

    `tests/test_judge_context.py`:
    - `test_build_tool_call_summary_no_calls` -- returns "No tool calls were made."
    - `test_build_tool_call_summary_truncates_long_args` -- args > 100 chars are truncated with "..."
    - `test_build_context_default` -- contains final response and tool call sections
    - `test_build_context_empty_response` -- "(empty)" when final_content is None
    - `test_build_context_with_system_prompt` -- includes system prompt and tools sections when opt-in
    - `test_build_context_truncates_long_system_prompt` -- system prompt > 2000 chars is truncated

    `tests/test_judge_extraction.py`:
    - `test_extract_from_tool_call_valid` -- scores correctly parsed from tool call arguments
    - `test_extract_from_tool_call_clamps_out_of_range` -- score > 1.0 clamped to 1.0, score < 0.0 clamped to 0.0
    - `test_extract_from_tool_call_no_match` -- returns None when no score_criteria tool call
    - `test_extract_json_from_text_direct` -- parses clean JSON text
    - `test_extract_json_from_text_brace` -- extracts JSON from text with surrounding content
    - `test_extract_json_from_text_code_block` -- extracts from ```json code block
    - `test_extract_json_from_text_invalid` -- returns None for non-JSON text
    - `test_extract_scores_prefers_tool_call` -- tool call extraction used when available
    - `test_extract_scores_falls_back_to_text` -- text fallback when no tool calls
    - `test_extract_scores_both_fail` -- returns None when both strategies fail
  </action>
  <verify>
    cd /Users/apple/JHERLETH/agent-something && source .venv/bin/activate && python -m pytest tests/test_judge_prompt.py tests/test_judge_context.py tests/test_judge_extraction.py -v
  </verify>
  <done>
    JudgeConfig model added to ProjectConfig with adapter/model/k/temperature/max_tokens defaults. Assertion model extended with criteria/judge_model/k/include_system_prompt/custom_prompt optional fields. Judge prompt builder generates system prompt from criteria with 0.0-1.0 scale and builds tool schema. Context builder produces trace summary with opt-in system prompt. Extraction parses tool calls with text-JSON fallback and score clamping. All tests pass.
  </done>
</task>

<task type="auto">
  <name>Task 2: JudgeEvaluator with k-vote aggregation and registry integration</name>
  <files>
    src/salvo/evaluation/judge/aggregation.py
    src/salvo/evaluation/evaluators/judge.py
    src/salvo/evaluation/evaluators/__init__.py
    tests/test_judge_aggregation.py
    tests/test_judge_evaluator.py
  </files>
  <action>
    **k-vote aggregation (src/salvo/evaluation/judge/aggregation.py):**

    1. `def aggregate_k_votes(k_results: list[dict], criteria: list[dict], threshold: float) -> tuple[float, bool, list[dict]]` -- aggregate k judge votes into a single assertion result.
       - Collect per-criterion scores across k votes (list of scores per criterion name).
       - Compute median score per criterion using `statistics.median()`. If a criterion has no scores, use 0.0.
       - Compute weighted average across criteria: `sum(median * weight) / sum(weight)` (same formula as existing scorer). Guard against zero total weight.
       - Majority vote: for each k-result, compute its weighted average across criteria, check if >= threshold. Count passes. `passed = pass_count > len(k_results) / 2` (strict majority).
       - Build per-criterion details list: `[{name, median_score, all_scores, weight}]` for each criterion.
       - Return `(overall_score, passed, per_criterion_details)`.

    Edge cases:
    - k=1: majority formula works (need > 0.5, so 1 pass suffices).
    - All criteria scores 0.0: overall_score = 0.0, passed = False.
    - Empty k_results: overall_score = 0.0, passed = False, empty details.
    - Single criterion: weighted average = that criterion's median.

    **JudgeEvaluator (src/salvo/evaluation/evaluators/judge.py):**

    1. `class JudgeEvaluator(BaseEvaluator)` -- the main evaluator class.

    2. `def evaluate(self, trace: RunTrace, assertion: dict) -> EvalResult` -- sync entry point. Detects running loop: if no loop, calls `asyncio.run(self.evaluate_async(...))`. If already in a loop, raises RuntimeError directing caller to use evaluate_async.

    3. `async def evaluate_async(self, trace: RunTrace, assertion: dict) -> EvalResult` -- the real implementation:
       - Extract from assertion: `criteria`, `judge_model` (default "gpt-4o-mini"), `k` (default 3), `weight`, `required`, `include_system_prompt`, `custom_prompt`, `threshold` (from assertion if present, default 0.8).
       - Build context via `build_context(trace, scenario=None, include_system_prompt=include_sp)`. Note: scenario is not available in the assertion dict. For `include_system_prompt`, we can pass scenario=None and the context builder handles it gracefully (skips system prompt sections when scenario is None even if flag is True). OR, pass the scenario through the assertion dict. Research open question #2 suggests merging config into assertion dict upstream. For now: add an optional `_scenario` key to the assertion dict that the pipeline sets before calling evaluate. If not present, `include_system_prompt` effectively has no system prompt to include.
       - Build judge system prompt: `custom_prompt` if provided, else `build_judge_prompt(criteria)`.
       - Build scoring tool via `build_scoring_tool(criteria)`.
       - Resolve adapter: `get_adapter(assertion.get("judge_adapter", "openai"))`.
       - Determine provider name: `adapter.provider_name()`.
       - Build tool_choice extras: `format_tool_choice(provider_name, "score_criteria")`.
       - Build AdapterConfig with judge_model, temperature=0.0, max_tokens=1024, extras=tool_choice.
       - Run k judge calls in a loop:
         - Build messages: `[Message(role="system", content=system_prompt), Message(role="user", content=user_prompt)]` where user_prompt is built from JUDGE_USER_TEMPLATE with the context block.
         - `result = await adapter.send_turn(messages, tools=[scoring_tool], config=config)`
         - Extract scores via `extract_scores(result, criteria)`. If not None, append to k_results. Else increment parse_failures.
         - Track cost via `estimate_cost(judge_model, result.usage.input_tokens, result.usage.output_tokens)`. Accumulate total_judge_cost.
         - On exception: increment parse_failures, continue.
       - If k_results is empty (all failed): return EvalResult with assertion_type="judge", score=0.0, passed=False, details="judge_parse_failed: {parse_failures}/{k} calls failed".
       - Aggregate via `aggregate_k_votes(k_results, criteria, threshold)`.
       - Build details string: `"judge={model} k={k} votes={len(k_results)}/{k} | judge_cost=${cost:.6f} | {per_criterion_details}"`.
       - Return EvalResult with assertion_type="judge", score=overall_score, passed=majority_passed, weight, required, details.
       - Store judge cost in a structured way: add optional `metadata: dict | None = None` field to EvalResult (plan 05-02 will add this field). For now, encode cost in the details string.

    4. `def resolve_judge_config(assertion: dict, project_config: dict | None = None) -> dict` -- a helper function that merges project-level judge defaults into the assertion dict. Resolution order: assertion-level override > salvo.yaml judge section > hard-coded defaults. This is called by the evaluation pipeline before invoking the evaluator. Fields resolved: `judge_adapter` (default "openai"), `judge_model` (default "gpt-4o-mini"), `k` (default 3), temperature, max_tokens.

    **Registry update (src/salvo/evaluation/evaluators/__init__.py):**

    Add `from salvo.evaluation.evaluators.judge import JudgeEvaluator` and `"judge": JudgeEvaluator` to EVALUATOR_REGISTRY.

    **Tests (TDD -- write first):**

    `tests/test_judge_aggregation.py`:
    - `test_aggregate_single_vote_pass` -- k=1, single result above threshold, passes
    - `test_aggregate_single_vote_fail` -- k=1, single result below threshold, fails
    - `test_aggregate_three_votes_majority_pass` -- 2/3 above threshold, passes
    - `test_aggregate_three_votes_majority_fail` -- 1/3 above threshold, fails
    - `test_aggregate_median_robust_to_outlier` -- two scores at 0.8, one at 0.1 -> median 0.8
    - `test_aggregate_weighted_criteria` -- two criteria with different weights, verify weighted average
    - `test_aggregate_empty_results` -- empty k_results returns (0.0, False, [])
    - `test_aggregate_missing_criterion_scores` -- criterion missing from some votes gets 0.0 for missing
    - `test_aggregate_zero_weight_guard` -- all criteria weight 0, returns (0.0, False, ...)

    `tests/test_judge_evaluator.py`:
    - Use `AsyncMock` for adapter.send_turn to return pre-configured AdapterTurnResults with tool calls.
    - `test_judge_evaluator_registered` -- `get_evaluator("judge")` returns JudgeEvaluator instance
    - `test_evaluate_async_pass` -- mock adapter returns valid scores above threshold for 3 calls, verify passed=True
    - `test_evaluate_async_fail` -- mock adapter returns scores below threshold, verify passed=False
    - `test_evaluate_async_parse_failure_fallback` -- first call returns no tool calls but text JSON, verify text fallback works
    - `test_evaluate_async_all_parse_failures` -- all k calls return garbage, verify judge_parse_failed in details
    - `test_evaluate_async_cost_tracked` -- verify judge cost accumulated in details string
    - `test_resolve_judge_config_defaults` -- empty assertion gets hard-coded defaults
    - `test_resolve_judge_config_assertion_override` -- assertion-level model/k overrides project defaults
    - `test_resolve_judge_config_project_override` -- project config overrides hard-coded defaults

    For evaluator tests, mock `get_adapter` to return a mock adapter, mock `estimate_cost` to return known values. Use `asyncio.run()` or `pytest-asyncio` to run async tests.
  </action>
  <verify>
    cd /Users/apple/JHERLETH/agent-something && source .venv/bin/activate && python -m pytest tests/test_judge_aggregation.py tests/test_judge_evaluator.py -v
  </verify>
  <done>
    aggregate_k_votes computes median per-criterion scores and majority-vote pass/fail correctly for all edge cases. JudgeEvaluator registered as "judge" in EVALUATOR_REGISTRY. evaluate_async orchestrates k judge calls via adapter, extracts scores, aggregates, and returns EvalResult with judge cost in details. resolve_judge_config correctly merges assertion > project > hard-coded defaults. All tests pass.
  </done>
</task>

</tasks>

<verification>
```bash
cd /Users/apple/JHERLETH/agent-something && source .venv/bin/activate
python -m pytest tests/test_judge_prompt.py tests/test_judge_context.py tests/test_judge_extraction.py tests/test_judge_aggregation.py tests/test_judge_evaluator.py -v
python -c "from salvo.evaluation.evaluators import get_evaluator; e = get_evaluator('judge'); print(f'Judge evaluator: {type(e).__name__}')"
python -c "from salvo.models.config import ProjectConfig, JudgeConfig; c = ProjectConfig(); print(f'Judge defaults: adapter={c.judge.adapter} model={c.judge.model} k={c.judge.k}')"
python -c "from salvo.evaluation.judge.prompt import build_judge_prompt, build_scoring_tool; print('Judge prompt modules importable')"
python -c "from salvo.evaluation.judge.context import build_context; print('Context builder importable')"
python -c "from salvo.evaluation.judge.extraction import extract_scores; print('Extraction importable')"
python -c "from salvo.evaluation.judge.aggregation import aggregate_k_votes; print('Aggregation importable')"
```
</verification>

<success_criteria>
- JudgeConfig model with adapter/model/k/temperature/max_tokens defaults in ProjectConfig
- Assertion model accepts criteria, judge_model, k, include_system_prompt, custom_prompt for judge assertions
- Judge prompt auto-generated from criteria with 0.0-1.0 scale anchoring
- Scoring tool schema matches criteria with per-criterion score+reasoning properties
- Context builder produces trace summary with opt-in system prompt/tool definitions
- Structured extraction prefers tool_call, falls back to text-JSON, returns None on total failure
- k-vote aggregation uses median for robustness, majority vote for pass/fail
- JudgeEvaluator registered in EVALUATOR_REGISTRY as "judge"
- resolve_judge_config merges assertion > project > hard-coded defaults
- All tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/05-llm-judge-evaluation/05-01-SUMMARY.md`
</output>
