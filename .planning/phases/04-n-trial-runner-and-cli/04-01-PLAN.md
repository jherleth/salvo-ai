---
phase: 04-n-trial-runner-and-cli
plan: 01
type: tdd
wave: 1
depends_on: []
files_modified:
  - src/salvo/models/trial.py
  - src/salvo/execution/trial_runner.py
  - src/salvo/execution/retry.py
  - src/salvo/evaluation/aggregation.py
  - tests/test_trial_models.py
  - tests/test_aggregation.py
  - tests/test_retry.py
  - tests/test_trial_runner.py
autonomous: true
requirements: [EXEC-01, EXEC-02, EXEC-03, EVAL-08]

must_haves:
  truths:
    - "TrialRunner executes a scenario N times and produces a TrialSuiteResult with aggregate metrics"
    - "Each trial runs in an isolated temporary directory with a fresh adapter instance"
    - "Trials can run concurrently with bounded parallelism via asyncio.Semaphore + TaskGroup"
    - "Transient errors (timeout, 429, 5xx) are retried up to 3 times with exponential backoff + jitter"
    - "Cross-trial assertion failure counts are aggregated by expression with frequency and weighted impact ranking"
    - "Early stop aborts remaining trials when score threshold is unreachable or hard fail occurs"
  artifacts:
    - path: "src/salvo/models/trial.py"
      provides: "TrialResult, TrialSuiteResult, Verdict, TrialStatus Pydantic models"
      contains: "class TrialSuiteResult"
    - path: "src/salvo/execution/trial_runner.py"
      provides: "TrialRunner class orchestrating N-trial execution"
      contains: "class TrialRunner"
    - path: "src/salvo/execution/retry.py"
      provides: "Transient error retry with exponential backoff + jitter"
      contains: "async def retry_with_backoff"
    - path: "src/salvo/evaluation/aggregation.py"
      provides: "Aggregate metrics computation and cross-trial failure ranking"
      contains: "def compute_aggregate_metrics"
  key_links:
    - from: "src/salvo/execution/trial_runner.py"
      to: "src/salvo/execution/runner.py"
      via: "TrialRunner creates ScenarioRunner per trial"
      pattern: "ScenarioRunner\\(adapter\\)"
    - from: "src/salvo/execution/trial_runner.py"
      to: "src/salvo/evaluation/aggregation.py"
      via: "TrialRunner calls compute_aggregate_metrics on collected TrialResults"
      pattern: "compute_aggregate_metrics"
    - from: "src/salvo/execution/trial_runner.py"
      to: "src/salvo/execution/retry.py"
      via: "Each trial execution wrapped in retry_with_backoff"
      pattern: "retry_with_backoff"
---

<objective>
Build the N-trial execution engine: data models for trial results and suite aggregation, retry logic for transient errors, aggregate metrics computation with cross-trial failure ranking, and the TrialRunner class that orchestrates N isolated/concurrent trial executions.

Purpose: This is the core execution layer that transforms Salvo from a single-run tool into a reliability testing framework. Every other Phase 4 component (CLI output, report command) consumes the TrialSuiteResult produced here.
Output: TrialRunner class, data models, retry utility, aggregation functions, and comprehensive tests.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@src/salvo/models/result.py
@src/salvo/execution/runner.py
@src/salvo/evaluation/scorer.py
@src/salvo/adapters/base.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Trial data models and aggregate metrics computation</name>
  <files>
    src/salvo/models/trial.py
    src/salvo/evaluation/aggregation.py
    tests/test_trial_models.py
    tests/test_aggregation.py
  </files>
  <action>
    **Data models (src/salvo/models/trial.py):**

    Create Pydantic models following existing patterns (see models/result.py):

    1. `TrialStatus(str, Enum)` with values: `passed`, `failed`, `hard_fail`, `infra_error`
    2. `TrialResult(BaseModel)` with fields:
       - `trial_number: int`
       - `status: TrialStatus`
       - `score: float`
       - `passed: bool`
       - `eval_results: list[EvalResult] = []` (import from models.result)
       - `latency_seconds: float`
       - `cost_usd: float | None = None`
       - `retries_used: int = 0`
       - `transient_error_types: list[str] = Field(default_factory=list)`
       - `error_message: str | None = None` (for INFRA_ERROR trials)
       - `trace_id: str | None = None`
    3. `Verdict(str, Enum)` with values: `PASS`, `FAIL`, `HARD FAIL`, `PARTIAL`, `INFRA_ERROR`
    4. `TrialSuiteResult(BaseModel)` with fields:
       - `run_id: str`
       - `scenario_name: str`, `scenario_file: str`, `model: str`, `adapter: str`
       - `trials: list[TrialResult]`
       - `trials_total: int`, `trials_passed: int`, `trials_failed: int`, `trials_hard_fail: int`, `trials_infra_error: int`
       - `verdict: Verdict`, `pass_rate: float`
       - `score_avg: float`, `score_min: float`, `score_p50: float`, `score_p95: float`, `threshold: float`
       - `cost_total: float | None = None`, `cost_avg_per_trial: float | None = None`
       - `latency_p50: float | None = None`, `latency_p95: float | None = None`
       - `total_retries: int = 0`, `trials_with_retries: int = 0`
       - `early_stopped: bool = False`, `early_stop_reason: str | None = None`
       - `n_requested: int = 3`
       - `assertion_failures: list[dict] = Field(default_factory=list)` (aggregated failure ranking)

    All models use `extra='forbid'` per project convention.

    **Aggregation (src/salvo/evaluation/aggregation.py):**

    1. `compute_aggregate_metrics(scored_trials: list[TrialResult], threshold: float) -> dict` — compute score_avg, score_min, score_p50, score_p95, pass_rate, latency_p50, latency_p95, cost_total, cost_avg_per_trial. Use `statistics.quantiles(data, n=100)` for percentiles. Guard: if 0 scored trials return zeroed metrics; if 1 scored trial use that value for all percentiles (statistics.quantiles requires >= 2 points).

    2. `determine_verdict(trials: list[TrialResult], avg_score: float, threshold: float, allow_infra: bool = False) -> Verdict`:
       - Any INFRA_ERROR and not allow_infra -> INFRA_ERROR
       - Any hard_fail trials -> HARD FAIL
       - avg_score < threshold and pass_rate > 0 -> PARTIAL (at least one passed but overall fails)
       - avg_score < threshold and pass_rate == 0 -> FAIL
       - avg_score >= threshold and no hard fails -> PASS

    3. `aggregate_failures(trials: list[TrialResult]) -> list[dict]` — cross-trial assertion failure aggregation. For each failed assertion across all trials, collect: assertion_type, expression (from details[:80]), fail_count, fail_rate (fail_count / total trials), total_weight_lost ((1-score)*weight), sample_details (up to 3). Sort by fail_count * avg_weight_lost descending. Per user decision: rank by both frequency AND weighted impact.

    **Tests:**

    TDD style. Write tests first for:
    - TrialResult and TrialSuiteResult JSON round-trip serialization
    - compute_aggregate_metrics with 0, 1, 2, and 10 scored trials
    - determine_verdict for each verdict case (PASS, FAIL, HARD FAIL, PARTIAL, INFRA_ERROR)
    - aggregate_failures with mixed pass/fail trials, correct frequency and impact ranking
    - Edge cases: all INFRA_ERROR trials, single trial, all passing trials
  </action>
  <verify>
    cd /Users/apple/JHERLETH/agent-something && source .venv/bin/activate && python -m pytest tests/test_trial_models.py tests/test_aggregation.py -v
  </verify>
  <done>
    TrialResult and TrialSuiteResult serialize to/from JSON. compute_aggregate_metrics returns correct percentiles for all edge cases. determine_verdict maps all 5 verdict states correctly. aggregate_failures ranks failures by frequency * weighted impact.
  </done>
</task>

<task type="auto">
  <name>Task 2: TrialRunner with isolation, concurrency, retry, and early-stop</name>
  <files>
    src/salvo/execution/retry.py
    src/salvo/execution/trial_runner.py
    tests/test_retry.py
    tests/test_trial_runner.py
  </files>
  <action>
    **Retry utility (src/salvo/execution/retry.py):**

    Hand-rolled retry (no tenacity dependency — ~20 lines per research recommendation).

    1. `TRANSIENT_EXCEPTIONS = (TimeoutError, ConnectionError)` tuple.
    2. `TRANSIENT_STATUS_CODES = {429, 500, 502, 503}` frozenset.
    3. `_is_transient(exc: Exception) -> bool` — check isinstance against TRANSIENT_EXCEPTIONS, then check `getattr(exc, "status_code", None) or getattr(exc, "status", None)` against TRANSIENT_STATUS_CODES.
    4. `async def retry_with_backoff(coro_factory: Callable[[], Awaitable], max_retries: int = 3, base_delay: float = 1.0, max_delay: float = 30.0) -> tuple[Any, int, list[str]]` — returns (result, retries_used, error_types). Loop `max_retries + 1` times. On transient error: record type, compute delay = min(base_delay * 2^attempt, max_delay), jitter = random.uniform(0, delay), await asyncio.sleep(jitter). On non-transient or final attempt: raise.

    **TrialRunner (src/salvo/execution/trial_runner.py):**

    ```python
    class TrialRunner:
        def __init__(
            self,
            adapter_factory: Callable[[], BaseAdapter],
            scenario: Scenario,
            config: AdapterConfig,
            n_trials: int = 3,
            max_parallel: int = 1,  # 1 = sequential
            max_retries: int = 3,
            early_stop: bool = False,
            threshold: float = 1.0,
        ) -> None: ...
    ```

    Key design:
    - `adapter_factory` (NOT a shared adapter instance) — creates fresh adapter per trial to prevent shared client state (per pitfall #2 from research).
    - `async def run_all(self, progress_callback: Callable[[int, int], None] | None = None) -> TrialSuiteResult` — main entry point.
    - `async def _execute_single_trial(self, trial_num: int) -> TrialResult` — runs one trial:
      1. Create `tempfile.TemporaryDirectory(prefix=f"salvo_trial_{trial_num}_")` for isolation (per EXEC-02).
      2. Create fresh adapter via `self.adapter_factory()`.
      3. Create `ScenarioRunner(adapter)`.
      4. Wrap `runner.run(scenario, config)` in `retry_with_backoff()`.
      5. If retry succeeds: evaluate assertions via `evaluate_trace()`, build TrialResult with status/score/eval_results/retries_used/transient_error_types.
      6. If retries exhausted (raises): build TrialResult with status=INFRA_ERROR, error_message=str(exc).
      7. Call `progress_callback(trial_num, n_trials)` after completion.
    - Sequential mode (`max_parallel == 1`): simple for-loop, check early_stop after each trial.
    - Concurrent mode (`max_parallel > 1`): `asyncio.TaskGroup` + `asyncio.Semaphore(max_parallel)`. Use `asyncio.Event` for early-stop cancellation signal. Each task checks the event before starting execution; if set, skip and return a sentinel (don't count toward results).
    - Early-stop logic: after each completed trial, check if remaining trials cannot mathematically reach threshold (pessimistic: assume remaining all score 1.0, check if new avg still < threshold). Also stop on first HARD FAIL.
    - After all trials: call `compute_aggregate_metrics()`, `determine_verdict()`, `aggregate_failures()`, build and return `TrialSuiteResult`.
    - Use `uuid7()` for suite run_id. Save individual trial trace_ids for later retrieval.

    IMPORTANT anti-patterns to avoid:
    - Do NOT use `os.chdir()` for isolation (not async-safe). Use tmpdir as a parameter only.
    - Do NOT share adapter instances across trials.
    - Serialize RunStore writes after all trials complete (per pitfall #5).

    **Tests:**

    TDD style. Use a MockAdapter (from existing test patterns) that returns pre-configured AdapterTurnResults.

    Test cases:
    - `test_single_trial_pass` — N=1, adapter returns valid response, assertions pass -> PASS verdict
    - `test_multiple_trials_mixed` — N=5, some pass some fail -> correct pass_rate, score_avg, verdict
    - `test_concurrent_execution` — N=5, max_parallel=3, verify all 5 complete with correct results
    - `test_trial_isolation` — verify each trial gets a unique tmpdir (mock TemporaryDirectory to capture paths)
    - `test_retry_on_transient_error` — adapter raises TimeoutError first call, succeeds second -> retries_used=1
    - `test_retry_exhausted_infra_error` — adapter always raises 429 -> trial status=INFRA_ERROR
    - `test_early_stop_on_hard_fail` — N=10, first trial has hard fail with --early-stop -> stops after 1
    - `test_early_stop_mathematically_impossible` — N=10, first 5 all score 0.0 with threshold 0.8 and --early-stop -> stops before completing all 10
    - `test_progress_callback_called` — verify callback invoked once per trial with (trial_num, total)
    - `test_all_infra_error` — all trials fail with infra errors -> verdict=INFRA_ERROR, zeroed metrics
  </action>
  <verify>
    cd /Users/apple/JHERLETH/agent-something && source .venv/bin/activate && python -m pytest tests/test_retry.py tests/test_trial_runner.py -v
  </verify>
  <done>
    TrialRunner executes N trials with isolation (unique tmpdir + fresh adapter per trial). Concurrent mode uses Semaphore-bounded TaskGroup. Retry logic handles transient errors with backoff + jitter. Early stop aborts on hard fail or mathematical impossibility. All tests pass.
  </done>
</task>

</tasks>

<verification>
```bash
cd /Users/apple/JHERLETH/agent-something && source .venv/bin/activate
python -m pytest tests/test_trial_models.py tests/test_aggregation.py tests/test_retry.py tests/test_trial_runner.py -v
python -c "from salvo.models.trial import TrialResult, TrialSuiteResult, Verdict, TrialStatus; print('Models importable')"
python -c "from salvo.execution.trial_runner import TrialRunner; print('TrialRunner importable')"
python -c "from salvo.evaluation.aggregation import compute_aggregate_metrics, aggregate_failures; print('Aggregation importable')"
```
</verification>

<success_criteria>
- TrialRunner.run_all() executes N trials and returns a TrialSuiteResult with correct aggregate metrics
- Each trial runs in a unique temporary directory with a fresh adapter instance
- Concurrent mode (max_parallel > 1) produces same results as sequential for deterministic inputs
- Transient errors retried with exponential backoff + jitter; exhausted retries produce INFRA_ERROR status
- Early stop triggers on hard fail or mathematical impossibility
- Cross-trial failure aggregation ranks by frequency * weighted impact
- All tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/04-n-trial-runner-and-cli/04-01-SUMMARY.md`
</output>
