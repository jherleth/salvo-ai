---
phase: 04-n-trial-runner-and-cli
plan: 03
type: execute
wave: 2
depends_on: ["04-01"]
files_modified:
  - src/salvo/cli/report_cmd.py
  - src/salvo/cli/main.py
  - tests/test_report_cmd.py
autonomous: true
requirements: [CLI-03]

must_haves:
  truths:
    - "User can run `salvo report` and see the latest run's detailed results"
    - "User can run `salvo report --history` and see a trend table of recent runs (pass rate, score, cost over time)"
    - "User can filter results by failure type"
  artifacts:
    - path: "src/salvo/cli/report_cmd.py"
      provides: "salvo report command with latest run detail and history trend view"
      contains: "def report"
    - path: "src/salvo/cli/main.py"
      provides: "Report command registered in CLI"
      contains: "report"
  key_links:
    - from: "src/salvo/cli/report_cmd.py"
      to: "src/salvo/storage/json_store.py"
      via: "Report command reads suite results from RunStore"
      pattern: "load_suite_result|load_latest_suite|list_runs"
    - from: "src/salvo/cli/main.py"
      to: "src/salvo/cli/report_cmd.py"
      via: "Report command registered as Typer subcommand"
      pattern: "app\\.command.*report"
---

<objective>
Build the `salvo report` command that shows the latest run's detailed results by default, supports a `--history` trend view across recent runs, and provides failure filtering.

Purpose: Users need to review past run results without re-running scenarios. The report command provides post-hoc analysis of stored suite results, complementing the live `salvo run` output.
Output: report_cmd.py module, main.py integration, and tests.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-n-trial-runner-and-cli/04-01-SUMMARY.md
@src/salvo/cli/main.py
@src/salvo/storage/json_store.py
@src/salvo/models/trial.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: salvo report command with latest detail, history trend, and failure filtering</name>
  <files>
    src/salvo/cli/report_cmd.py
    src/salvo/cli/main.py
    tests/test_report_cmd.py
  </files>
  <action>
    **Create report_cmd.py (src/salvo/cli/report_cmd.py):**

    Import Rich components directly (Table, Console, Panel, box) — this module does its own rendering since report layout differs from live verdict rendering in output.py.

    **CLI function signature:**

    ```python
    def report(
        run_id: str = typer.Argument(None, help="Run ID to display (default: latest)"),
        history: bool = typer.Option(False, "--history", help="Show trend view of recent runs"),
        limit: int = typer.Option(10, "--limit", "-l", help="Number of runs to show in history"),
        scenario: str = typer.Option(None, "--scenario", "-s", help="Filter by scenario name"),
        failures_only: bool = typer.Option(False, "--failures", help="Show only failed assertions"),
    ) -> None:
    ```

    **Default mode (latest run detail):**

    1. Resolve run: if `run_id` provided, load that suite result. Otherwise load latest via `store.load_latest_suite()`.
    2. If no runs found: print `[dim]No runs found. Run 'salvo run' first.[/dim]` and exit.
    3. Display run detail using Rich:
       - Header: scenario name, model, adapter, timestamp, run_id
       - Verdict line with color styling (same symbols as output.py: ✓ ✗ ! ~)
       - Metrics table: trials passed/total, pass rate, score avg/min/p50/p95, cost, latency
       - Per-trial breakdown: Rich Table with columns [Trial#, Status, Score, Latency, Cost, Retries]
       - Assertion failures section (if any): assertion type, expression, fail count, weight impact
       - If `--failures` flag: filter to show only failed assertions from assertion_failures list

    **History mode (`--history`):**

    1. Load run IDs: `store.list_runs(scenario_name=scenario)`. Take last `limit` runs.
    2. Load each suite result via `store.load_suite_result(run_id)`. Skip any that fail to load (corrupted, or old RunResult format — catch ValidationError and skip with warning).
    3. Render trend table with Rich Table:
       ```
       ┌─────────┬──────────────┬─────────┬───────┬────────┬───────────┐
       │ Run ID  │ Scenario     │ Verdict │ Score │ Trials │ Cost      │
       ├─────────┼──────────────┼─────────┼───────┼────────┼───────────┤
       │ abc1... │ my_scenario  │ ✓ PASS  │ 0.95  │ 3/3    │ $0.0042   │
       │ def2... │ my_scenario  │ ✗ FAIL  │ 0.62  │ 1/3    │ $0.0038   │
       └─────────┴──────────────┴─────────┴───────┴────────┴───────────┘
       ```
       Columns: Run ID (truncated to 8 chars), Scenario, Verdict (styled), Score, Trials (passed/total), Cost, Date
    4. If `--failures` flag with `--history`: only show runs with non-PASS verdict.
    5. Summary line at bottom: `{N} runs shown. Pass rate trend: {first_pass_rate:.0%} → {latest_pass_rate:.0%}`

    **Error handling:**

    - RunStore not initialized (no .salvo/): print helpful message and exit.
    - Run ID not found: print `Run '{run_id}' not found.` and list available run IDs.
    - No runs matching scenario filter: print `No runs found for scenario '{scenario}'.`

    **Register in main.py:**

    Add `from salvo.cli.report_cmd import report as report_cmd` and `app.command(name="report")(report_cmd)`.
    Use `name="report"` to register under the `report` subcommand name.

    **Tests (tests/test_report_cmd.py):**

    Use `tmp_path` fixture to create a RunStore with pre-saved TrialSuiteResult fixtures.

    Test cases:
    - `test_report_latest` — save a suite result, run report with no args, verify output contains scenario name and verdict
    - `test_report_by_run_id` — save two suite results, request specific run_id, verify correct one displayed
    - `test_report_no_runs` — empty store, verify helpful message
    - `test_report_history` — save 3 suite results, run `--history`, verify trend table with 3 rows
    - `test_report_history_limit` — save 5 results, run `--history --limit 2`, verify only 2 shown
    - `test_report_scenario_filter` — save results for 2 different scenarios, filter by one, verify only matching shown
    - `test_report_failures_only` — suite with mixed pass/fail assertions, `--failures` flag filters to failed only
    - Use `typer.testing.CliRunner` to invoke commands and capture output
  </action>
  <verify>
    cd /Users/apple/JHERLETH/agent-something && source .venv/bin/activate && python -m pytest tests/test_report_cmd.py -v
    python -c "from salvo.cli.report_cmd import report; print('report_cmd importable')"
    python -c "from salvo.cli.main import app; print('main.py updated')"
  </verify>
  <done>
    `salvo report` shows latest run detail with verdict, metrics, per-trial breakdown, and assertion failures. `salvo report --history` shows trend table of recent runs with pass rate, score, and cost. `--failures` flag filters to failed assertions/runs only. `--scenario` filters by scenario name. `--limit` controls history count. Command registered in main.py and all tests pass.
  </done>
</task>

</tasks>

<verification>
```bash
cd /Users/apple/JHERLETH/agent-something && source .venv/bin/activate
python -m pytest tests/test_report_cmd.py -v
python -c "from salvo.cli.main import app; print([c.name for c in app.registered_commands])"
```
</verification>

<success_criteria>
- `salvo report` displays latest run detail with all metrics and per-trial breakdown
- `salvo report --history` displays trend table of recent runs
- `salvo report --failures` filters to failed assertions/runs only
- `salvo report --scenario NAME` filters by scenario name
- Report command registered in main.py and accessible via `salvo report`
- Graceful handling of empty store, missing run IDs, corrupted files
- All tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/04-n-trial-runner-and-cli/04-03-SUMMARY.md`
</output>
