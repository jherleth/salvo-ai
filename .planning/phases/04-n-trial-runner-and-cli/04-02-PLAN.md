---
phase: 04-n-trial-runner-and-cli
plan: 02
type: execute
wave: 2
depends_on: ["04-01"]
files_modified:
  - src/salvo/cli/output.py
  - src/salvo/cli/run_cmd.py
  - src/salvo/storage/json_store.py
  - src/salvo/cli/main.py
  - tests/test_output.py
  - tests/test_run_cmd_ntrial.py
autonomous: true
requirements: [CLI-02, CLI-06]

must_haves:
  truths:
    - "User sees a Rich progress bar during trial execution and a compact headline verdict table after completion"
    - "User can pass -n, --parallel, --json, --verbose, --early-stop, --allow-infra flags to salvo run"
    - "CLI exits with code 0 (PASS), 1 (FAIL), 2 (HARD FAIL), or 3 (INFRA ERROR)"
    - "Machine-readable JSON is always written to .salvo/runs/<run_id>.json with a latest symlink"
    - "--json flag outputs pure JSON to stdout with no Rich markup"
    - "Detail sections (top offenders, score breakdown, latency, cost, sample failures) shown on failure or --verbose"
  artifacts:
    - path: "src/salvo/cli/output.py"
      provides: "Rich verdict rendering: headline table, detail sections, progress bar, JSON output"
      contains: "def render_headline"
    - path: "src/salvo/cli/run_cmd.py"
      provides: "Refactored run command with N-trial integration and new CLI flags"
      contains: "TrialRunner"
    - path: "src/salvo/storage/json_store.py"
      provides: "save_suite_result and update_latest_symlink methods"
      contains: "def save_suite_result"
  key_links:
    - from: "src/salvo/cli/run_cmd.py"
      to: "src/salvo/execution/trial_runner.py"
      via: "run command creates and runs TrialRunner"
      pattern: "TrialRunner.*run_all"
    - from: "src/salvo/cli/run_cmd.py"
      to: "src/salvo/cli/output.py"
      via: "run command calls render functions for verdict display"
      pattern: "render_headline|render_details"
    - from: "src/salvo/cli/run_cmd.py"
      to: "src/salvo/storage/json_store.py"
      via: "run command persists TrialSuiteResult and updates latest symlink"
      pattern: "save_suite_result"
---

<objective>
Build the Rich terminal output layer and refactor `salvo run` to use the TrialRunner, with full CLI flag support, verdict-styled output, JSON mode for CI, suite result persistence with latest symlink, and exit codes 0/1/2/3.

Purpose: This is the user-facing layer that makes N-trial results readable and actionable, both for interactive terminal use and CI pipeline integration.
Output: output.py rendering module, refactored run_cmd.py, extended RunStore, and tests.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-n-trial-runner-and-cli/04-01-SUMMARY.md
@src/salvo/cli/run_cmd.py
@src/salvo/cli/main.py
@src/salvo/storage/json_store.py
@src/salvo/models/trial.py
@src/salvo/execution/trial_runner.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Rich verdict rendering module (output.py)</name>
  <files>
    src/salvo/cli/output.py
    tests/test_output.py
  </files>
  <action>
    Create `src/salvo/cli/output.py` — the Rich rendering layer for N-trial results.

    **Progress bar function:**

    `create_trial_progress(console: Console) -> Progress` — returns a Rich Progress instance with custom columns:
    - `TextColumn("[bold blue]{task.description}")` — shows "Running trials"
    - `BarColumn()` — visual progress bar
    - `MofNCompleteColumn()` — "7/20" counter
    - `TimeElapsedColumn()` — elapsed time
    - `transient=True` — disappear after completion to make room for results
    - Only show when `console.is_terminal` is True. If not terminal, return None (caller must handle).

    **Headline table:**

    `render_headline(suite: TrialSuiteResult, console: Console) -> None` — compact ~5-8 line table per user decision:

    Verdict styling (per locked decision):
    - PASS: `[bold green]✓ PASS[/bold green]`
    - FAIL: `[bold red]✗ FAIL[/bold red]`
    - HARD FAIL: `[bold bright_red]! HARD FAIL[/bold bright_red]`
    - PARTIAL: `[bold yellow]~ PARTIAL[/bold yellow]`
    - INFRA_ERROR: `[bold bright_red]! INFRA ERROR[/bold bright_red]`

    Use `rich.table.Table(box=box.SIMPLE, show_header=False, padding=(0, 2))` with two columns (Key bold, Value).
    Rows:
    1. Verdict: styled verdict string
    2. Trials: `{passed}/{total} passed ({pass_rate:.0%})`
    3. Score: `avg={score_avg:.2f} min={score_min:.2f} p50={score_p50:.2f} p95={score_p95:.2f} (threshold={threshold:.2f})`
    4. Failures: `{hard_fail_count} hard fail, {soft_fail_count} soft fail` (only if any failures)
    5. Latency: `p50={latency_p50:.2f}s p95={latency_p95:.2f}s` (only if available)
    6. Cost: `total=${cost_total:.4f} avg=${cost_avg_per_trial:.4f}/trial` (only if available)
    7. Transient: `{total_retries} retries across {trials_with_retries} trials` (only if retries > 0)
    8. Infra errors: `{trials_infra_error} trial(s) (excluded from score)` (only if > 0)

    If `suite.early_stopped`: add row `Status: FAIL (early stop after {actual}/{requested} trials)`

    **Detail sections:**

    `render_details(suite: TrialSuiteResult, console: Console) -> None` — shown when verdict != PASS or --verbose.

    Ordered per user decision:
    1. **Top offenders** — from `suite.assertion_failures` list. Render as numbered list:
       `1. {assertion_type}: {expression} — failed {fail_count}/{total} ({fail_rate:.0%}), weight impact: {avg_weight_lost:.2f}`
       Show up to 5 top offenders.

    2. **Score breakdown** — per-trial scores as a horizontal distribution:
       `Scores: 1.0, 0.8, 0.6, 0.4, 1.0` (one per trial, excluding INFRA_ERROR)

    3. **Latency distribution** — `min={min:.2f}s p50={p50:.2f}s p95={p95:.2f}s max={max:.2f}s`

    4. **Cost breakdown** — `total=${total:.4f} min=${min:.4f} max=${max:.4f} avg=${avg:.4f}/trial`

    5. **Sample failures** — 1-3 representative failure details from assertion_failures[0].sample_details. Truncate model response excerpts to 200 chars.

    **JSON output:**

    `output_json(suite: TrialSuiteResult) -> None` — writes `suite.model_dump_json(indent=2)` to stdout. No Rich, no color, no extra text.

    **Non-TTY / CI behavior:**

    When `console.is_terminal` is False or `NO_COLOR` env is set:
    - Skip progress bar entirely
    - Strip Rich markup from verdict (keep symbols: ✓ ✗ ! ~)
    - Still render headline and details as plain text via `console.print()` (Rich handles stripping automatically when color disabled)

    **Tests (tests/test_output.py):**

    - Test render_headline produces correct key/value rows for each verdict type
    - Test render_details includes top offenders when failures exist
    - Test output_json writes valid JSON to stdout
    - Test non-TTY mode strips color (use `Console(force_terminal=False)`)
    - Use `Console(file=StringIO())` to capture Rich output for assertions
  </action>
  <verify>
    cd /Users/apple/JHERLETH/agent-something && source .venv/bin/activate && python -m pytest tests/test_output.py -v
  </verify>
  <done>
    render_headline displays compact verdict table with correct styling for all 5 verdict types. render_details shows top offenders, score breakdown, latency, cost, and sample failures. output_json writes pure JSON to stdout. Non-TTY mode works without ANSI garbage.
  </done>
</task>

<task type="auto">
  <name>Task 2: Refactor run_cmd.py with TrialRunner integration and suite persistence</name>
  <files>
    src/salvo/cli/run_cmd.py
    src/salvo/storage/json_store.py
    src/salvo/cli/main.py
    tests/test_run_cmd_ntrial.py
  </files>
  <action>
    **Extend RunStore (src/salvo/storage/json_store.py):**

    Add two methods:

    1. `save_suite_result(self, suite: TrialSuiteResult) -> str` — persist the entire TrialSuiteResult as a single JSON file at `.salvo/runs/{run_id}.json`. Use atomic write (existing pattern). Update index under scenario_name. Return run_id.

    2. `update_latest_symlink(self, run_id: str) -> None` — create/update a `latest` symlink in `.salvo/runs/` pointing to `{run_id}.json`. Use atomic pattern: `os.symlink(target, tmp_link)` then `os.replace(tmp_link, link_path)`. Catch OSError for Windows compatibility: fall back to writing a `.latest` text file containing the run_id.

    3. `load_suite_result(self, run_id: str) -> TrialSuiteResult` — load a TrialSuiteResult from its JSON file. Used by report command (04-03).

    4. `load_latest_suite(self) -> TrialSuiteResult | None` — read the `latest` symlink (or `.latest` fallback file), then load that suite result. Return None if no latest exists.

    Import TrialSuiteResult from salvo.models.trial. Add to existing imports.

    **Refactor run_cmd.py (src/salvo/cli/run_cmd.py):**

    Replace the entire `run()` and `_run_async()` functions. The new `run()` function accepts:
    - `scenario_path: str` (positional, required)
    - `-n` / `--trials`: int, default=3 per locked decision
    - `--parallel`: int, default=1 (sequential by default, per locked decision)
    - `--json` / `--format-json`: bool, default=False — pure JSON output mode
    - `--verbose` / `-V`: bool, default=False — show detail sections even on pass
    - `--early-stop`: bool, default=False — stop when threshold unreachable or hard fail
    - `--allow-infra`: bool, default=False — exit 0/1/2 instead of 3 on infra errors
    - `--threshold`: float | None — override scenario threshold (optional)

    New `_run_async()` flow:

    1. Load and validate scenario (same as current steps 1-3).
    2. Build AdapterConfig (same as current step 4).
    3. Create adapter_factory: `lambda: get_adapter(scenario.adapter)` — returns fresh adapter per trial.
    4. Determine threshold: `--threshold` flag overrides `scenario.threshold`.
    5. Create TrialRunner with adapter_factory, scenario, config, n_trials, max_parallel, max_retries=3, early_stop, threshold.
    6. If not `--json` and console.is_terminal: create Rich Progress, run trials with progress callback that calls `progress.update(task, advance=1)`.
    7. If `--json` or not terminal: run trials with no progress callback.
    8. Get TrialSuiteResult from `runner.run_all()`.
    9. Persist: `store.save_suite_result(suite)`, `store.update_latest_symlink(suite.run_id)`. Also save individual trial traces for trials with trace_ids: `store.save_trace(trace_id, trace)`.
    10. If `--json`: call `output_json(suite)` and exit.
    11. Otherwise: call `render_headline(suite, output_console)`. If verdict != PASS or --verbose: call `render_details(suite, output_console)`.
    12. Print `[dim]Run saved: {suite.run_id}[/dim]`.
    13. Exit with code from EXIT_CODES dict: `{"PASS": 0, "FAIL": 1, "HARD FAIL": 2, "PARTIAL": 1, "INFRA_ERROR": 3}`. PARTIAL exits with 1 (score < threshold). If --allow-infra and verdict is INFRA_ERROR: use scored-trials-based verdict instead but print warning.

    Keep `_find_project_root()` unchanged.

    Note: The current single-run flow (steps 5-14 in current run_cmd.py) is fully replaced. N=1 produces a TrialSuiteResult with one trial (per research recommendation: always produce TrialSuiteResult for consistency).

    **Update main.py:**

    No changes needed for this task — `run` command is already registered. The signature change (adding typer.Option params) is handled by Typer automatically.

    **Tests (tests/test_run_cmd_ntrial.py):**

    - Test that `--json` flag produces valid JSON on stdout and no Rich markup
    - Test exit code 0 for PASS verdict, 1 for FAIL, 2 for HARD FAIL, 3 for INFRA_ERROR
    - Test `--allow-infra` changes exit code from 3 to scored verdict code
    - Test latest symlink created after run
    - Test TrialSuiteResult saved to .salvo/runs/ and loadable
    - Use MockAdapter and tmp_path fixtures for isolation
  </action>
  <verify>
    cd /Users/apple/JHERLETH/agent-something && source .venv/bin/activate && python -m pytest tests/test_run_cmd_ntrial.py tests/test_output.py -v
    python -c "from salvo.cli.run_cmd import run; print('run_cmd importable')"
    python -c "from salvo.storage.json_store import RunStore; print('RunStore importable')"
  </verify>
  <done>
    `salvo run` uses TrialRunner to execute N trials (default 3) with progress bar. All CLI flags work: -n, --parallel, --json, --verbose, --early-stop, --allow-infra. Exit codes 0/1/2/3 map correctly to verdicts. Suite results persisted with latest symlink. JSON output mode produces pure JSON on stdout.
  </done>
</task>

</tasks>

<verification>
```bash
cd /Users/apple/JHERLETH/agent-something && source .venv/bin/activate
python -m pytest tests/test_output.py tests/test_run_cmd_ntrial.py -v
python -c "from salvo.cli.output import render_headline, render_details, output_json; print('output.py importable')"
python -c "from salvo.cli.run_cmd import run; print('run_cmd refactored')"
```
</verification>

<success_criteria>
- Rich progress bar shown during trial execution in TTY mode, suppressed in non-TTY/--json mode
- Headline table displays verdict with correct color/symbol styling for all 5 verdict types
- Detail sections render top offenders, score breakdown, latency, cost, and sample failures
- --json flag outputs pure JSON to stdout with no extra text
- Exit codes: 0=PASS, 1=FAIL/PARTIAL, 2=HARD FAIL, 3=INFRA_ERROR
- TrialSuiteResult persisted to .salvo/runs/ with atomic writes and latest symlink
- All tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/04-n-trial-runner-and-cli/04-02-SUMMARY.md`
</output>
