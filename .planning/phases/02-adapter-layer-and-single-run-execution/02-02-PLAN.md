---
phase: 02-adapter-layer-and-single-run-execution
plan: 02
type: execute
wave: 2
depends_on:
  - 02-01
files_modified:
  - src/salvo/models/scenario.py
  - src/salvo/models/__init__.py
  - src/salvo/adapters/openai_adapter.py
  - src/salvo/adapters/anthropic_adapter.py
  - src/salvo/adapters/__init__.py
  - pyproject.toml
  - tests/test_models_scenario.py
  - tests/test_adapters_openai.py
  - tests/test_adapters_anthropic.py
autonomous: true
requirements:
  - ADPT-01
  - ADPT-02

user_setup:
  - service: openai
    why: "OpenAI adapter needs API access for integration testing"
    env_vars:
      - name: OPENAI_API_KEY
        source: "OpenAI Dashboard -> API Keys (https://platform.openai.com/api-keys)"
  - service: anthropic
    why: "Anthropic adapter needs API access for integration testing"
    env_vars:
      - name: ANTHROPIC_API_KEY
        source: "Anthropic Console -> API Keys (https://console.anthropic.com/settings/keys)"

must_haves:
  truths:
    - "OpenAI adapter converts unified Messages to OpenAI chat format and back"
    - "Anthropic adapter converts unified Messages to Anthropic messages format and back"
    - "OpenAI adapter extracts tool calls from response and returns AdapterTurnResult"
    - "Anthropic adapter extracts tool_use blocks from response and returns AdapterTurnResult"
    - "Both adapters read API keys from environment variables (not constructor params)"
    - "Both adapters report token usage in AdapterTurnResult.usage"
    - "Scenario model accepts max_turns, temperature, seed, and extras fields"
  artifacts:
    - path: "src/salvo/adapters/openai_adapter.py"
      provides: "OpenAIAdapter with message/tool format conversion and usage extraction"
      contains: "class OpenAIAdapter(BaseAdapter)"
    - path: "src/salvo/adapters/anthropic_adapter.py"
      provides: "AnthropicAdapter with message/tool format conversion and usage extraction"
      contains: "class AnthropicAdapter(BaseAdapter)"
    - path: "src/salvo/models/scenario.py"
      provides: "Extended Scenario model with max_turns, temperature, seed, extras"
      contains: "max_turns"
    - path: "pyproject.toml"
      provides: "Optional dependencies for openai and anthropic SDKs"
      contains: "openai"
  key_links:
    - from: "src/salvo/adapters/openai_adapter.py"
      to: "src/salvo/adapters/base.py"
      via: "subclass of BaseAdapter, implements send_turn()"
      pattern: "class OpenAIAdapter\\(BaseAdapter\\)"
    - from: "src/salvo/adapters/anthropic_adapter.py"
      to: "src/salvo/adapters/base.py"
      via: "subclass of BaseAdapter, implements send_turn()"
      pattern: "class AnthropicAdapter\\(BaseAdapter\\)"
    - from: "src/salvo/adapters/openai_adapter.py"
      to: "openai.AsyncOpenAI"
      via: "lazy-initialized async client"
      pattern: "AsyncOpenAI"
    - from: "src/salvo/adapters/anthropic_adapter.py"
      to: "anthropic.AsyncAnthropic"
      via: "lazy-initialized async client"
      pattern: "AsyncAnthropic"
---

<objective>
Extend the Scenario model with Phase 2 execution fields and implement both concrete provider adapters (OpenAI and Anthropic) that normalize provider-specific API formats into the unified BaseAdapter interface.

Purpose: Without concrete adapters, scenarios cannot be executed against real LLMs. Without model extensions, execution parameters cannot be configured per scenario.
Output: Extended Scenario model, OpenAIAdapter, AnthropicAdapter, optional SDK dependencies in pyproject.toml.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-adapter-layer-and-single-run-execution/02-RESEARCH.md
@.planning/phases/02-adapter-layer-and-single-run-execution/02-01-SUMMARY.md
@src/salvo/models/scenario.py
@src/salvo/adapters/base.py
@src/salvo/adapters/registry.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Extend Scenario model and add optional SDK dependencies</name>
  <files>
    src/salvo/models/scenario.py
    pyproject.toml
    tests/test_models_scenario.py
  </files>
  <action>
    Add four optional fields to the Scenario model in src/salvo/models/scenario.py:
    - `max_turns: int = Field(default=10, ge=1, le=100)` -- Safety net for multi-turn loop. Default 10 per locked decision.
    - `temperature: float | None = None` -- Override adapter default. None means use provider default.
    - `seed: int | None = None` -- For reproducibility where supported.
    - `extras: dict[str, Any] = Field(default_factory=dict)` -- Provider-specific options dict passed through to API call.

    These fields maintain `extra='forbid'` on the model (existing pattern) so unknown YAML keys are still rejected.

    Update pyproject.toml to add optional dependencies:
    ```toml
    [project.optional-dependencies]
    openai = ["openai>=2.21.0"]
    anthropic = ["anthropic>=0.80.0"]
    all = ["openai>=2.21.0", "anthropic>=0.80.0"]
    dev = [
        "pytest>=8.0",
        "pytest-cov",
        "pytest-asyncio>=0.25.0",
    ]
    ```

    Add tests to existing tests/test_models_scenario.py:
    - test_scenario_default_max_turns (10)
    - test_scenario_custom_max_turns
    - test_scenario_max_turns_validation (ge=1, le=100)
    - test_scenario_temperature_optional
    - test_scenario_seed_optional
    - test_scenario_extras_default_empty
    - test_scenario_extras_accepts_dict
    - test_scenario_round_trip_with_new_fields (JSON serialize/deserialize preserves all fields)

    Install the optional dependencies in the dev venv:
    ```bash
    pip install -e ".[all,dev]"
    ```
  </action>
  <verify>
    cd /Users/apple/JHERLETH/agent-something && .venv/bin/python -m pytest tests/test_models_scenario.py -v
  </verify>
  <done>
    Scenario model accepts max_turns, temperature, seed, and extras. Defaults are sensible (max_turns=10, others None/empty). Validation rejects max_turns < 1 or > 100. All existing scenario tests still pass. Optional SDK dependencies declared in pyproject.toml and installed.
  </done>
</task>

<task type="auto">
  <name>Task 2: OpenAI and Anthropic adapter implementations</name>
  <files>
    src/salvo/adapters/openai_adapter.py
    src/salvo/adapters/anthropic_adapter.py
    src/salvo/adapters/__init__.py
    tests/test_adapters_openai.py
    tests/test_adapters_anthropic.py
  </files>
  <action>
    **OpenAI Adapter (src/salvo/adapters/openai_adapter.py):**

    Create OpenAIAdapter(BaseAdapter) with:
    - Lazy-initialized `AsyncOpenAI()` client (reads OPENAI_API_KEY from env automatically)
    - `async send_turn(messages, tools, config) -> AdapterTurnResult` that:
      1. Converts unified Messages to OpenAI format via `_convert_messages()`:
         - system -> {"role": "system", "content": ...}
         - user -> {"role": "user", "content": ...}
         - assistant -> {"role": "assistant", "content": ..., "tool_calls": [...]} (tool_calls as {"id", "type": "function", "function": {"name", "arguments": json.dumps()}})
         - tool_result -> {"role": "tool", "tool_call_id": ..., "content": ...}
      2. Converts tool defs via `_convert_tools()`:
         - {"type": "function", "function": {"name", "description", "parameters"}}
      3. Builds kwargs: model, messages, tools (if any), temperature (if set), max_tokens (if set), seed (if set), **extras
      4. Calls `client.chat.completions.create(**kwargs)`
      5. Extracts from response:
         - content from choice.message.content
         - tool_calls: parse each tc.function.arguments via json.loads into ToolCallResult
         - usage: prompt_tokens, completion_tokens, total_tokens
         - finish_reason from choice.finish_reason
         - raw_response via response.model_dump()
    - `provider_name() -> "openai"`

    **Anthropic Adapter (src/salvo/adapters/anthropic_adapter.py):**

    Create AnthropicAdapter(BaseAdapter) with:
    - Lazy-initialized `AsyncAnthropic()` client (reads ANTHROPIC_API_KEY from env automatically)
    - `async send_turn(messages, tools, config) -> AdapterTurnResult` that:
      1. Extracts system prompt from messages (Anthropic uses separate `system` param, NOT a message)
      2. Converts remaining Messages to Anthropic format via `_convert_message()`:
         - user -> {"role": "user", "content": ...}
         - assistant -> {"role": "assistant", "content": [{"type": "text", "text": ...}, {"type": "tool_use", "id": ..., "name": ..., "input": ...}]}
         - tool_result -> {"role": "user", "content": [{"type": "tool_result", "tool_use_id": ..., "content": ...}]}
         - CRITICAL: Do NOT add text blocks after tool_result content blocks in the same message (causes empty responses from Claude)
      3. Converts tool defs via `_convert_tools()`:
         - {"name", "description", "input_schema"} (NOT "parameters")
      4. Builds kwargs: model, messages, max_tokens (required, default 4096), system (if present), tools (if any), temperature (if set), **extras
      5. Calls `client.messages.create(**kwargs)`
      6. Extracts from response:
         - content: join text blocks from response.content
         - tool_calls: extract tool_use blocks into ToolCallResult (block.input is already a dict, no json.loads needed)
         - usage: input_tokens, output_tokens, total = input + output
         - finish_reason from response.stop_reason (NOT "finish_reason")
         - raw_response via response.model_dump()
    - `provider_name() -> "anthropic"`

    **Update src/salvo/adapters/__init__.py** to re-export OpenAIAdapter and AnthropicAdapter (with try/except for optional imports).

    **Tests (mock-based, no real API calls):**

    Both test files use unittest.mock to mock the SDK clients. This ensures tests run in CI without API keys.

    tests/test_adapters_openai.py:
    - test_openai_convert_messages_system: system message -> {"role": "system", ...}
    - test_openai_convert_messages_tool_result: tool_result -> {"role": "tool", "tool_call_id": ..., ...}
    - test_openai_convert_messages_assistant_with_tool_calls: includes tool_calls array
    - test_openai_convert_tools: tool defs -> OpenAI function format
    - test_openai_send_turn_text_response: mock client returns text, verify AdapterTurnResult fields
    - test_openai_send_turn_tool_call_response: mock client returns tool_calls, verify extraction
    - test_openai_send_turn_usage_extraction: verify token counts mapped correctly
    - test_openai_send_turn_extras_passed_through: verify extras end up in kwargs
    - test_openai_provider_name: returns "openai"
    - test_openai_lazy_client_init: client not created until first send_turn

    tests/test_adapters_anthropic.py:
    - test_anthropic_convert_messages_extracts_system: system message removed from messages, passed as system param
    - test_anthropic_convert_messages_tool_result: tool_result -> user message with tool_result content block
    - test_anthropic_convert_messages_assistant_with_tool_use: includes tool_use content blocks
    - test_anthropic_convert_tools: tool defs -> Anthropic format (input_schema, not parameters)
    - test_anthropic_send_turn_text_response: mock client returns text block, verify extraction
    - test_anthropic_send_turn_tool_use_response: mock client returns tool_use block, verify extraction (arguments already dict)
    - test_anthropic_send_turn_usage_extraction: verify input_tokens, output_tokens, total computed
    - test_anthropic_send_turn_max_tokens_default: verify 4096 default when config.max_tokens is None
    - test_anthropic_send_turn_extras_passed_through: verify extras end up in kwargs
    - test_anthropic_provider_name: returns "anthropic"

    For mocking, create realistic mock response objects that match the SDK's response structure:
    - OpenAI: Mock ChatCompletion with choices[0].message.content, choices[0].message.tool_calls, choices[0].finish_reason, usage.prompt_tokens/completion_tokens/total_tokens
    - Anthropic: Mock Message with content=[TextBlock/ToolUseBlock], stop_reason, usage.input_tokens/output_tokens, model_dump()
  </action>
  <verify>
    cd /Users/apple/JHERLETH/agent-something && .venv/bin/python -m pytest tests/test_adapters_openai.py tests/test_adapters_anthropic.py -v
  </verify>
  <done>
    OpenAI adapter correctly converts unified messages/tools to OpenAI format and extracts results. Anthropic adapter correctly converts to Anthropic format (system as param, input_schema not parameters, tool_result in user message). Both adapters lazy-init their clients, read API keys from env, pass extras through, and report usage. All tests pass with mocked SDK clients.
  </done>
</task>

</tasks>

<verification>
```bash
cd /Users/apple/JHERLETH/agent-something && .venv/bin/python -m pytest tests/test_models_scenario.py tests/test_adapters_openai.py tests/test_adapters_anthropic.py -v
```

All tests pass. Scenario model extended. Both adapters handle format conversion correctly.
</verification>

<success_criteria>
1. Scenario model has max_turns (default 10), temperature, seed, extras fields with validation
2. OpenAI adapter converts messages/tools to OpenAI format, extracts tool_calls and usage from responses
3. Anthropic adapter converts messages/tools to Anthropic format (system as param, input_schema, tool_result in user role), extracts tool_use blocks and usage
4. Both adapters use lazy-init async clients reading API keys from env
5. Both adapters pass extras through to API calls
6. pyproject.toml has optional openai, anthropic, and all dependency groups
7. All new and existing tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/02-adapter-layer-and-single-run-execution/02-02-SUMMARY.md`
</output>
