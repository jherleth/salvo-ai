---
phase: 02-adapter-layer-and-single-run-execution
plan: 03
type: execute
wave: 3
depends_on:
  - 02-01
  - 02-02
files_modified:
  - src/salvo/execution/runner.py
  - src/salvo/execution/trace.py
  - src/salvo/execution/redaction.py
  - src/salvo/execution/__init__.py
  - src/salvo/cli/run_cmd.py
  - src/salvo/cli/main.py
  - tests/test_execution_runner.py
  - tests/test_execution_trace.py
  - tests/test_execution_redaction.py
  - tests/test_cli_run.py
autonomous: true
requirements:
  - EXEC-04

must_haves:
  truths:
    - "ScenarioRunner drives a multi-turn conversation loop feeding mock tool responses back to the adapter"
    - "Runner stops when model produces final answer (no tool calls) or max turns is hit"
    - "Runner fails immediately if model calls a tool with no mock_response defined"
    - "Runner processes ALL tool calls from a single response (handles parallel tool calls)"
    - "Token usage is accumulated across all turns, not just the last"
    - "Run trace contains all messages, tool calls, turn count, usage, latency, cost estimate, scenario hash, and provider metadata"
    - "Stored traces have redaction applied (secret patterns replaced) and size limits enforced"
    - "salvo run command loads scenario, resolves adapter, executes, prints summary, and stores result"
  artifacts:
    - path: "src/salvo/execution/runner.py"
      provides: "ScenarioRunner with async run() method driving multi-turn loop"
      contains: "class ScenarioRunner"
    - path: "src/salvo/execution/trace.py"
      provides: "RunTrace and TraceMessage dataclasses for replay-ready trace storage"
      contains: "class RunTrace"
    - path: "src/salvo/execution/redaction.py"
      provides: "redact_content() and truncate_content() for trace safety"
      exports: ["redact_content", "truncate_content"]
    - path: "src/salvo/cli/run_cmd.py"
      provides: "salvo run CLI command wiring scenario loading, adapter resolution, execution, and storage"
      contains: "def run"
  key_links:
    - from: "src/salvo/execution/runner.py"
      to: "src/salvo/adapters/base.py"
      via: "calls adapter.send_turn() in loop"
      pattern: "adapter\\.send_turn"
    - from: "src/salvo/execution/runner.py"
      to: "src/salvo/execution/cost.py"
      via: "estimate_cost() on accumulated usage"
      pattern: "estimate_cost"
    - from: "src/salvo/execution/runner.py"
      to: "src/salvo/execution/extras.py"
      via: "validate_extras() before passing to adapter"
      pattern: "validate_extras"
    - from: "src/salvo/cli/run_cmd.py"
      to: "src/salvo/execution/runner.py"
      via: "ScenarioRunner.run() call"
      pattern: "ScenarioRunner"
    - from: "src/salvo/cli/run_cmd.py"
      to: "src/salvo/storage/json_store.py"
      via: "RunStore.save_run() for persistence"
      pattern: "RunStore"
    - from: "src/salvo/cli/run_cmd.py"
      to: "src/salvo/adapters/registry.py"
      via: "get_adapter() for adapter resolution"
      pattern: "get_adapter"
---

<objective>
Build the execution pipeline: ScenarioRunner multi-turn loop with mock tool injection, replay-ready trace models with redaction, and the `salvo run` CLI command that ties everything together.

Purpose: This is the end-to-end execution path -- the core deliverable of Phase 2. After this plan, users can run `salvo run scenario.yaml` and get a complete trace with cost, latency, and metadata.
Output: ScenarioRunner, trace models, redaction utilities, `salvo run` CLI command with summary output and RunStore persistence.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-adapter-layer-and-single-run-execution/02-RESEARCH.md
@.planning/phases/02-adapter-layer-and-single-run-execution/02-01-SUMMARY.md
@.planning/phases/02-adapter-layer-and-single-run-execution/02-02-SUMMARY.md
@src/salvo/adapters/base.py
@src/salvo/adapters/registry.py
@src/salvo/execution/cost.py
@src/salvo/execution/extras.py
@src/salvo/models/scenario.py
@src/salvo/models/result.py
@src/salvo/storage/json_store.py
@src/salvo/cli/main.py
@src/salvo/loader/validator.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Trace models, redaction utilities, and ScenarioRunner</name>
  <files>
    src/salvo/execution/trace.py
    src/salvo/execution/redaction.py
    src/salvo/execution/runner.py
    src/salvo/execution/__init__.py
    tests/test_execution_trace.py
    tests/test_execution_redaction.py
    tests/test_execution_runner.py
  </files>
  <action>
    **Trace models (src/salvo/execution/trace.py):**

    Create Pydantic models (NOT dataclasses -- these get serialized to JSON for storage, Pydantic gives us model_dump/model_validate for free):

    ```python
    class TraceMessage(BaseModel):
        role: str  # system, user, assistant, tool_result
        content: str | None = None
        tool_calls: list[dict[str, Any]] | None = None  # Serialized ToolCallResult
        tool_call_id: str | None = None
        tool_name: str | None = None

    class RunTrace(BaseModel):
        messages: list[TraceMessage]
        tool_calls_made: list[dict[str, Any]]  # All tool calls across turns
        turn_count: int
        input_tokens: int
        output_tokens: int
        total_tokens: int
        latency_seconds: float
        final_content: str | None
        finish_reason: str
        model: str
        provider: str
        timestamp: datetime
        scenario_hash: str  # SHA-256 of resolved scenario
        cost_usd: float | None = None
        extras_resolved: dict[str, Any] = Field(default_factory=dict)
        max_turns_hit: bool = False  # True if terminated by safety net
    ```

    **Redaction (src/salvo/execution/redaction.py):**

    ```python
    REDACTION_PATTERNS = [
        r'(?i)(api[_-]?key|secret|password|token|authorization)\s*[:=]\s*\S+',
        r'sk-[a-zA-Z0-9]{20,}',          # OpenAI API key pattern
        r'(?i)bearer\s+[a-zA-Z0-9._-]+',  # Bearer tokens
    ]

    MAX_MESSAGE_CONTENT_SIZE = 50_000    # 50KB per message
    MAX_RAW_RESPONSE_SIZE = 100_000      # 100KB per raw response
    MAX_TRACE_TOTAL_SIZE = 5_000_000     # 5MB total trace

    def redact_content(content: str) -> str: ...
    def truncate_content(content: str, max_size: int) -> str: ...
    def apply_trace_limits(trace: RunTrace) -> RunTrace: ...
    ```

    `apply_trace_limits` iterates over trace.messages, applies redact_content + truncate_content to each message's content, and truncates tool_calls dicts if they're too large.

    **ScenarioRunner (src/salvo/execution/runner.py):**

    Create custom exception: `class ToolMockNotFoundError(Exception)` with tool_name and available_mocks attributes.

    ScenarioRunner class:
    - `__init__(self, adapter: BaseAdapter)` -- stores adapter reference
    - `async run(self, scenario: Scenario, config: AdapterConfig) -> RunTrace`:
      1. Read max_turns from scenario (default 10)
      2. Build initial messages: system prompt (if any) + user prompt
      3. Build tool_defs list from scenario.tools (name, description, parameters.model_dump())
      4. Build mock_responses map: {tool.name: tool.mock_response for tool in scenario.tools if tool.mock_response is not None}
      5. Initialize: total_usage (TokenUsage), all_tool_calls list, start_time (time.perf_counter), turn_count = 0
      6. Loop up to max_turns:
         a. turn_count += 1
         b. `result = await self.adapter.send_turn(messages, tool_defs, config)`
         c. Accumulate result.usage into total_usage
         d. Append assistant Message to messages (with content and tool_calls)
         e. If no tool_calls in result: break (final answer)
         f. For EACH tool call in result.tool_calls (handle parallel tool calls):
            - If tc.name not in mock_responses: raise ToolMockNotFoundError
            - Serialize mock (json.dumps if dict, str otherwise)
            - Append tool_result Message with tool_call_id and content
         g. Add tool calls to all_tool_calls list
      7. elapsed = time.perf_counter() - start_time
      8. Determine max_turns_hit = (turn_count >= max_turns and result.tool_calls not empty)
      9. Compute scenario_hash: hashlib.sha256(scenario.model_dump_json().encode()).hexdigest()
      10. Compute cost_usd via estimate_cost()
      11. Build and return RunTrace with all fields

    Build Message -> TraceMessage conversion helper for trace construction.

    **Tests:**

    tests/test_execution_trace.py:
    - test_trace_message_serialization
    - test_run_trace_round_trip (model_dump_json + model_validate_json)
    - test_run_trace_defaults

    tests/test_execution_redaction.py:
    - test_redact_api_key_pattern
    - test_redact_openai_key_pattern (sk-...)
    - test_redact_bearer_token
    - test_redact_no_match_passthrough
    - test_truncate_within_limit
    - test_truncate_exceeds_limit
    - test_apply_trace_limits_redacts_messages

    tests/test_execution_runner.py (using mock adapter):
    - test_runner_single_turn_text_response: adapter returns text, runner stops, trace has 1 turn
    - test_runner_multi_turn_with_mock_tools: adapter returns tool_call, runner injects mock, adapter then returns text. Trace has 2 turns.
    - test_runner_tool_mock_not_found: adapter calls tool with no mock, raises ToolMockNotFoundError
    - test_runner_max_turns_safety_net: adapter always returns tool_calls, runner stops at max_turns
    - test_runner_parallel_tool_calls: adapter returns 2 tool_calls at once, both get mock responses
    - test_runner_accumulates_usage: multi-turn, usage is sum of all turns
    - test_runner_scenario_hash_deterministic: same scenario -> same hash
    - test_runner_cost_estimation: verify cost_usd is set for known models
    - test_runner_extras_validated: extras with blocked key raises ValueError

    For mock adapter in tests: create a `MockAdapter(BaseAdapter)` that returns configurable responses per turn. Use a list of pre-defined AdapterTurnResults that get popped on each send_turn call.
  </action>
  <verify>
    cd /Users/apple/JHERLETH/agent-something && .venv/bin/python -m pytest tests/test_execution_trace.py tests/test_execution_redaction.py tests/test_execution_runner.py -v
  </verify>
  <done>
    ScenarioRunner drives multi-turn conversation with mock tool injection. Stops on final answer or max turns. Fails on missing mock. Handles parallel tool calls. Accumulates usage. Computes cost, hash, and latency. Traces are redacted and size-limited. All runner tests pass with mock adapter.
  </done>
</task>

<task type="auto">
  <name>Task 2: salvo run CLI command with summary output and RunStore persistence</name>
  <files>
    src/salvo/cli/run_cmd.py
    src/salvo/cli/main.py
    tests/test_cli_run.py
  </files>
  <action>
    **CLI command (src/salvo/cli/run_cmd.py):**

    Create `run` function for Typer:

    ```python
    def run(
        scenario_path: str = typer.Argument(..., help="Path to scenario YAML file"),
    ) -> None:
        """Run a scenario against an LLM and display results."""
        asyncio.run(_run_async(scenario_path))
    ```

    `async _run_async(scenario_path: str)`:
    1. Load and validate scenario using `validate_scenario_file()` from salvo.loader.validator
       - If validation errors, print them with Rich console and exit(1)
    2. Resolve adapter via `get_adapter(scenario.adapter)` from salvo.adapters.registry
       - If import error (missing SDK), print helpful message and exit(1)
    3. Validate extras via `validate_extras(scenario.extras)` from salvo.execution.extras
       - If validation error, print message and exit(1)
    4. Build AdapterConfig from scenario fields: model, temperature, max_tokens (None for now, let provider default), seed, extras
    5. Create ScenarioRunner(adapter) and call `trace = await runner.run(scenario, config)`
    6. Apply trace limits: `trace = apply_trace_limits(trace)` from salvo.execution.redaction
    7. Print structured summary to terminal using Rich:
       ```
       Scenario: {scenario.description or scenario_path.stem}
       Model:    {trace.model} ({trace.provider})
       Turns:    {trace.turn_count}{" (max turns hit)" if trace.max_turns_hit else ""}
       Tools:    {len(trace.tool_calls_made)} call(s)
       Tokens:   {trace.input_tokens} in / {trace.output_tokens} out
       Cost:     ${trace.cost_usd:.6f} (estimated){" or "unknown" if None}
       Latency:  {trace.latency_seconds:.2f}s
       Hash:     {trace.scenario_hash[:12]}...
       ```
    8. Build RunResult from trace for persistence:
       - run_id: generate with uuid7()
       - metadata: RunMetadata from trace fields (scenario_name from path, scenario_file, scenario_hash, timestamp, model, adapter name, salvo_version, passed=True for now (no assertions yet), score=0.0, cost_usd, latency_seconds)
       - eval_results: [] (assertions evaluated in Phase 3)
       - score: 0.0 (Phase 3)
       - passed: True (no assertions to fail yet)
       - trace_id: same as run_id (trace stored inline)
    9. Store via RunStore: determine project root, save_run(run_result)
    10. Print: "Run saved: {run_id}"
    11. Handle ToolMockNotFoundError: catch, print descriptive error about missing mock, exit(1)
    12. Handle API errors from providers: catch Exception from adapter, print error, exit(1)

    **Register command in src/salvo/cli/main.py:**
    Import and register: `app.command()(run)` from `salvo.cli.run_cmd`

    **Tests (tests/test_cli_run.py):**

    Use typer.testing.CliRunner to invoke the CLI. Mock the adapter to avoid real API calls.

    - test_run_command_with_mock_adapter: Create a temp scenario YAML, mock get_adapter to return MockAdapter, invoke `salvo run scenario.yaml`, assert exit code 0, assert summary output contains model name, turn count, cost
    - test_run_command_invalid_scenario: Create invalid YAML, invoke, assert exit code 1 with error message
    - test_run_command_missing_sdk: Mock get_adapter to raise ImportError, assert exit code 1 with install hint
    - test_run_command_tool_mock_not_found: Mock adapter to call unknown tool, assert exit code 1 with error about missing mock
    - test_run_command_saves_to_store: After successful run, verify .salvo/runs/ contains a JSON file

    For mocking: use `unittest.mock.patch("salvo.cli.run_cmd.get_adapter")` to inject MockAdapter. Create temp scenario files using tmp_path fixture.
  </action>
  <verify>
    cd /Users/apple/JHERLETH/agent-something && .venv/bin/python -m pytest tests/test_cli_run.py -v
  </verify>
  <done>
    `salvo run scenario.yaml` loads scenario, resolves adapter, executes multi-turn loop, prints structured summary with turn count/cost/latency/hash, saves RunResult to .salvo/. Errors (invalid YAML, missing SDK, missing mock, API errors) produce clear messages and non-zero exit codes.
  </done>
</task>

</tasks>

<verification>
```bash
# Run all Phase 2 tests
cd /Users/apple/JHERLETH/agent-something && .venv/bin/python -m pytest tests/test_adapters_base.py tests/test_adapters_registry.py tests/test_execution_cost.py tests/test_execution_extras.py tests/test_adapters_openai.py tests/test_adapters_anthropic.py tests/test_execution_trace.py tests/test_execution_redaction.py tests/test_execution_runner.py tests/test_cli_run.py -v

# Run ALL tests (Phase 1 + Phase 2) to verify no regressions
cd /Users/apple/JHERLETH/agent-something && .venv/bin/python -m pytest -v
```

All Phase 2 tests pass. All Phase 1 tests still pass (no regressions).
</verification>

<success_criteria>
1. ScenarioRunner drives multi-turn loop: sends to adapter, checks for tool calls, injects mocks, repeats until final answer or max turns
2. Runner fails on missing mock with ToolMockNotFoundError
3. Runner handles parallel tool calls (all tool calls from one response get mocked)
4. Usage accumulated across turns (not overwritten)
5. RunTrace includes all messages, tool_calls, turn_count, usage, latency, cost, hash, provider metadata, max_turns_hit flag
6. Traces are redacted (secrets removed) and size-limited before storage
7. `salvo run` command works end-to-end: load -> validate -> resolve adapter -> execute -> print summary -> store
8. Error cases produce clear messages and exit code 1
9. All Phase 1 and Phase 2 tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/02-adapter-layer-and-single-run-execution/02-03-SUMMARY.md`
</output>
