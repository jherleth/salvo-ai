---
phase: 03-assertion-engine-and-scoring
plan: 01
type: tdd
wave: 1
depends_on: []
files_modified:
  - src/salvo/evaluation/__init__.py
  - src/salvo/evaluation/normalizer.py
  - src/salvo/evaluation/evaluators/__init__.py
  - src/salvo/evaluation/evaluators/base.py
  - src/salvo/evaluation/evaluators/jmespath_eval.py
  - src/salvo/evaluation/evaluators/tool_sequence.py
  - src/salvo/evaluation/evaluators/cost_limit.py
  - src/salvo/evaluation/evaluators/latency_limit.py
  - src/salvo/evaluation/scorer.py
  - tests/test_evaluation_normalizer.py
  - tests/test_evaluation_jmespath.py
  - tests/test_evaluation_tool_sequence.py
  - tests/test_evaluation_limits.py
  - tests/test_evaluation_scorer.py
  - pyproject.toml
autonomous: true
requirements: [EVAL-01, EVAL-02, EVAL-03, EVAL-04, EVAL-05, EVAL-06]

must_haves:
  truths:
    - "Normalizer converts operator-key-style assertions ({path, contains, ...}) to canonical form ({type, path, operator, value})"
    - "JMESPath evaluator queries trace data and applies all 8 comparison operators (eq, ne, gt, gte, lt, lte, contains, regex)"
    - "Tool sequence evaluator validates tool call sequences in EXACT, IN_ORDER, and ANY_ORDER modes with divergence-pinpointing failure messages"
    - "Cost limit evaluator passes when cost <= max_usd and fails when cost > max_usd or cost is unknown"
    - "Latency limit evaluator passes when latency <= max_seconds and fails when latency > max_seconds"
    - "Scorer computes weighted score as sum(score*weight)/sum(weight) and scenario passes when score >= threshold"
    - "Required assertions that fail cause hard_fail=True regardless of weighted score"
  artifacts:
    - path: "src/salvo/evaluation/normalizer.py"
      provides: "Assertion shorthand to canonical form conversion"
      contains: "OPERATOR_KEYS"
    - path: "src/salvo/evaluation/evaluators/base.py"
      provides: "BaseEvaluator ABC"
      exports: ["BaseEvaluator"]
    - path: "src/salvo/evaluation/evaluators/jmespath_eval.py"
      provides: "JMESPath query evaluator with 8 comparison operators"
      exports: ["JMESPathEvaluator"]
    - path: "src/salvo/evaluation/evaluators/tool_sequence.py"
      provides: "Tool call sequence validation in 3 modes"
      exports: ["ToolSequenceEvaluator"]
    - path: "src/salvo/evaluation/evaluators/cost_limit.py"
      provides: "Cost threshold evaluator"
      exports: ["CostLimitEvaluator"]
    - path: "src/salvo/evaluation/evaluators/latency_limit.py"
      provides: "Latency threshold evaluator"
      exports: ["LatencyLimitEvaluator"]
    - path: "src/salvo/evaluation/scorer.py"
      provides: "Weighted scoring with threshold and required assertion logic"
      exports: ["compute_score"]
  key_links:
    - from: "src/salvo/evaluation/evaluators/__init__.py"
      to: "evaluator classes"
      via: "EVALUATOR_REGISTRY dict mapping type strings to evaluator classes"
      pattern: "EVALUATOR_REGISTRY"
    - from: "src/salvo/evaluation/evaluators/jmespath_eval.py"
      to: "jmespath library"
      via: "jmespath.search() for path queries"
      pattern: "jmespath\\.search"
    - from: "src/salvo/evaluation/scorer.py"
      to: "salvo.models.result.EvalResult"
      via: "accepts list[EvalResult] and returns score/passed/hard_fail"
      pattern: "EvalResult"
---

<objective>
Build the assertion evaluation core: normalizer, four evaluators (JMESPath, tool sequence, cost limit, latency limit), evaluator registry, and weighted scorer with required-assertion hard-fail logic. All components are pure functions with clear I/O contracts, ideal for TDD.

Purpose: These components form the evaluation engine that takes a run trace and assertion definitions and produces scored results. They are the computational heart of Phase 3 â€” the integration plan (03-02) wires them into the CLI pipeline.
Output: Fully tested evaluation package with normalizer, 4 evaluators, registry, and scorer.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-assertion-engine-and-scoring/03-RESEARCH.md
@src/salvo/models/result.py
@src/salvo/models/scenario.py
@src/salvo/execution/trace.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Assertion normalizer, BaseEvaluator, registry, and all four evaluators</name>
  <files>
    src/salvo/evaluation/__init__.py
    src/salvo/evaluation/normalizer.py
    src/salvo/evaluation/evaluators/__init__.py
    src/salvo/evaluation/evaluators/base.py
    src/salvo/evaluation/evaluators/jmespath_eval.py
    src/salvo/evaluation/evaluators/tool_sequence.py
    src/salvo/evaluation/evaluators/cost_limit.py
    src/salvo/evaluation/evaluators/latency_limit.py
    tests/test_evaluation_normalizer.py
    tests/test_evaluation_jmespath.py
    tests/test_evaluation_tool_sequence.py
    tests/test_evaluation_limits.py
    pyproject.toml
  </files>
  <action>
    **TDD: RED phase first, then GREEN.**

    **Add jmespath dependency:** Add `"jmespath>=1.1.0"` to `[project] dependencies` in `pyproject.toml`. Run `pip install -e ".[dev]"` to install.

    **Normalizer (src/salvo/evaluation/normalizer.py):**
    - Define `OPERATOR_KEYS = {"eq", "ne", "gt", "gte", "lt", "lte", "contains", "regex"}`.
    - `normalize_assertion(raw: dict) -> dict`: If dict has `type` key, pass through (already canonical). Otherwise detect exactly one operator key from OPERATOR_KEYS in the dict's keys. Extract it as `operator`, its value as `value`, `path` as `expression` (default `path` to `response.content` if not provided). Build canonical form: `{type: "jmespath", expression: ..., operator: ..., value: ..., weight: ..., required: ...}`. Carry over `weight` (default 1.0) and `required` (default False). Error if multiple operator keys found. Error if no type and no operator key found.
    - `normalize_assertions(raw_list: list[dict]) -> list[dict]`: Map normalize_assertion over the list.

    **BaseEvaluator (src/salvo/evaluation/evaluators/base.py):**
    - ABC with one abstract method: `evaluate(self, trace: RunTrace, assertion: dict) -> EvalResult`.
    - Import from `salvo.execution.trace.RunTrace` and `salvo.models.result.EvalResult`.

    **Evaluator Registry (src/salvo/evaluation/evaluators/__init__.py):**
    - `EVALUATOR_REGISTRY: dict[str, type[BaseEvaluator]]` mapping `"jmespath"`, `"tool_sequence"`, `"cost_limit"`, `"latency_limit"` to their evaluator classes.
    - `get_evaluator(assertion_type: str) -> BaseEvaluator`: Look up class, instantiate, return. Raise ValueError for unknown types with list of available types.

    **JMESPath Evaluator (src/salvo/evaluation/evaluators/jmespath_eval.py):**
    - `build_trace_data(trace: RunTrace) -> dict`: Convert RunTrace to queryable dict with structure: `response` (content, finish_reason from final assistant message), `turns` (list of message dicts), `tool_calls` (list of tool call dicts from trace.tool_calls_made), `metadata` (model, provider, cost_usd, latency_seconds, input_tokens, output_tokens, total_tokens, turn_count, finish_reason). The `response.content` comes from `trace.final_content`.
    - `compare(actual, operator: str, expected) -> bool`: Implement 8 operators: `eq` (==), `ne` (!=), `gt` (float(actual) > float(expected)), `gte` (>=), `lt` (<), `lte` (<=), `contains` (str `in` for strings, `in` for lists), `regex` (re.search(str(expected), str(actual))). Return False if actual is None (path not found). Catch ValueError/TypeError for numeric operators and return False. Catch re.error for regex and return False.
    - `JMESPathEvaluator.evaluate()`: Extract `expression`, `operator`, `value` from assertion dict. Call `jmespath.search(expression, data)` where data = `build_trace_data(trace)`. Handle `jmespath.exceptions.ParseError` by returning EvalResult(score=0.0, passed=False, details=error message). Apply `compare()`. Return EvalResult with assertion_type="jmespath", score 1.0/0.0, passed, weight from assertion, required from assertion, details showing expected vs actual and path.

    **Tool Sequence Evaluator (src/salvo/evaluation/evaluators/tool_sequence.py):**
    - Extract tool call names from trace: `actual = [tc["name"] for tc in trace.tool_calls_made]`.
    - Three match functions returning `tuple[bool, str]`:
      - `match_exact(actual, expected)`: Lists must be equal. On failure, pinpoint divergence position, show expected/actual, show missing or extra items.
      - `match_in_order(actual, expected)`: Expected is a subsequence of actual (gaps allowed). Linear scan with index pointer. On failure, show which expected calls were matched and where matching stalled.
      - `match_any_order(actual, expected)`: All expected calls present using count comparison. On failure, show which calls are missing with expected/actual counts.
    - Zero tool calls: If `len(actual) == 0 and len(expected) > 0`, return `(False, "No tool calls made -- expected [...]")`.
    - `ToolSequenceEvaluator.evaluate()`: Extract `mode` (lowercase) and `sequence` from assertion dict. Dispatch to correct match function. Return EvalResult with assertion_type="tool_sequence".

    **Cost Limit Evaluator (src/salvo/evaluation/evaluators/cost_limit.py):**
    - `CostLimitEvaluator.evaluate()`: Extract `max_usd` from assertion dict. Check `trace.cost_usd`. If None, return EvalResult(score=0.0, passed=False, details="Cost unknown..."). Otherwise compare cost_usd <= max_usd. Return EvalResult with assertion_type="cost_limit", score 1.0/0.0, details showing actual vs limit.

    **Latency Limit Evaluator (src/salvo/evaluation/evaluators/latency_limit.py):**
    - `LatencyLimitEvaluator.evaluate()`: Extract `max_seconds` from assertion dict. Compare `trace.latency_seconds <= max_seconds`. Return EvalResult with assertion_type="latency_limit", score 1.0/0.0, details showing actual vs limit.

    **Tests (RED first, then GREEN):**

    `tests/test_evaluation_normalizer.py`:
    - Test operator-key shorthand with each of the 8 operators expands correctly
    - Test explicit `type` key passes through unchanged
    - Test default path is "response.content" when path omitted
    - Test weight and required carry over from raw dict
    - Test error on multiple operator keys
    - Test error on no type and no operator key
    - Test normalize_assertions maps over list

    `tests/test_evaluation_jmespath.py`:
    - Test build_trace_data produces correct structure from a RunTrace
    - Test each of the 8 comparison operators (eq, ne, gt, gte, lt, lte, contains, regex) with passing and failing cases
    - Test None actual (path not found) returns False for all operators
    - Test contains on list vs string
    - Test numeric type coercion in gt/gte/lt/lte (int vs float)
    - Test invalid JMESPath expression returns score=0.0 with error in details
    - Test invalid regex pattern returns score=0.0
    - Test full evaluate() flow: build trace -> query path -> compare -> EvalResult

    `tests/test_evaluation_tool_sequence.py`:
    - Test EXACT match success and failure (divergence point, too few, too many)
    - Test IN_ORDER match success (with gaps) and failure (stalled matching)
    - Test ANY_ORDER match success and failure (missing calls with counts)
    - Test zero tool calls with expected sequence gives distinct error message
    - Test evaluate() dispatches to correct match function based on mode

    `tests/test_evaluation_limits.py`:
    - Test cost_limit passes when cost <= max_usd
    - Test cost_limit fails when cost > max_usd
    - Test cost_limit fails with descriptive message when cost_usd is None
    - Test latency_limit passes when latency <= max_seconds
    - Test latency_limit fails when latency > max_seconds
    - Test weight and required carried from assertion dict to EvalResult

    **Package init (src/salvo/evaluation/__init__.py):**
    - Re-export: `normalize_assertion`, `normalize_assertions`, `get_evaluator`, `BaseEvaluator`, `build_trace_data`.
  </action>
  <verify>
    Run `cd /Users/apple/JHERLETH/agent-something && .venv/bin/python -m pytest tests/test_evaluation_normalizer.py tests/test_evaluation_jmespath.py tests/test_evaluation_tool_sequence.py tests/test_evaluation_limits.py -v` -- all tests pass.
    Run `cd /Users/apple/JHERLETH/agent-something && .venv/bin/python -m pytest tests/ -v` -- all existing tests still pass (zero regressions).
  </verify>
  <done>
    Normalizer converts all 8 operator-key shorthand forms to canonical. JMESPath evaluator handles all 8 operators with None/type-error/parse-error edge cases. Tool sequence evaluator validates EXACT/IN_ORDER/ANY_ORDER with divergence-pinpointing messages. Cost/latency evaluators check thresholds with None-safe handling. Registry resolves all 4 evaluator types. All tests pass.
  </done>
</task>

<task type="auto">
  <name>Task 2: Weighted scorer with required-assertion hard-fail logic</name>
  <files>
    src/salvo/evaluation/scorer.py
    tests/test_evaluation_scorer.py
    src/salvo/evaluation/__init__.py
  </files>
  <action>
    **TDD: RED phase first, then GREEN.**

    **Scorer (src/salvo/evaluation/scorer.py):**
    - `compute_score(eval_results: list[EvalResult], threshold: float) -> tuple[float, bool, bool]`:
      Returns `(score, passed, hard_fail)`.
      - If `eval_results` is empty, return `(1.0, True, False)` -- vacuous pass (no assertions = pass).
      - Check for hard_fail: `any(r.required and not r.passed for r in eval_results)`.
      - Compute weighted score: `sum(r.score * r.weight for r in eval_results) / sum(r.weight for r in eval_results)`.
      - Guard against zero total weight: if `total_weight == 0`, return `(0.0, False, hard_fail)`.
      - Compute passed: `score >= threshold and not hard_fail`.
      - Return `(score, passed, hard_fail)`.

    - `evaluate_trace(trace: RunTrace, assertions: list[dict], threshold: float) -> tuple[list[EvalResult], float, bool]`:
      Orchestration function that ties evaluators and scorer together.
      - Build trace data once via `build_trace_data(trace)`.
      - For each assertion in the normalized list:
        - Get evaluator via `get_evaluator(assertion["type"])`.
        - Call `evaluator.evaluate(trace, assertion)`.
        - Collect EvalResult.
      - Call `compute_score(eval_results, threshold)`.
      - Return `(eval_results, score, passed)`.
      Note: This function takes already-normalized assertions. Normalization happens upstream in the pipeline (03-02 handles this). The function also passes the full trace (not just trace data) to evaluators, since cost_limit and latency_limit read from trace directly.

    **Tests (RED first, then GREEN):**

    `tests/test_evaluation_scorer.py`:
    - Test empty eval_results returns (1.0, True, False)
    - Test all passing assertions with equal weights
    - Test mixed pass/fail with equal weights gives correct weighted average
    - Test unequal weights affect score correctly (e.g., weight 3.0 vs 1.0)
    - Test score >= threshold passes, score < threshold fails
    - Test exact threshold boundary (score == threshold passes)
    - Test required assertion failure causes hard_fail=True and passed=False even when score >= threshold
    - Test zero total weight returns (0.0, False, ...)
    - Test evaluate_trace end-to-end: create a RunTrace, define normalized assertions, verify eval_results + score + passed

    **Update evaluation __init__.py:**
    - Add re-exports for `compute_score` and `evaluate_trace`.
  </action>
  <verify>
    Run `cd /Users/apple/JHERLETH/agent-something && .venv/bin/python -m pytest tests/test_evaluation_scorer.py -v` -- all tests pass.
    Run `cd /Users/apple/JHERLETH/agent-something && .venv/bin/python -m pytest tests/ -v` -- all tests pass (zero regressions).
  </verify>
  <done>
    compute_score correctly implements weighted average formula sum(score*weight)/sum(weight). Threshold comparison works including boundary. Required assertion hard-fail overrides score. evaluate_trace orchestrates the full pipeline from trace + assertions to scored results. All tests pass.
  </done>
</task>

</tasks>

<verification>
- All evaluation module files exist under src/salvo/evaluation/
- jmespath>=1.1.0 added to pyproject.toml dependencies
- EVALUATOR_REGISTRY maps 4 type strings to evaluator classes
- normalize_assertion handles all 8 operator keys plus passthrough
- All new tests pass and all existing tests pass (zero regressions)
</verification>

<success_criteria>
- Normalizer converts operator-key shorthand to canonical form for all 8 operators
- JMESPath evaluator queries RunTrace data and applies all 8 comparison operators
- Tool sequence evaluator validates EXACT, IN_ORDER, ANY_ORDER modes with divergence messages
- Cost and latency evaluators check thresholds with None-safe cost handling
- Scorer computes weighted average with required-assertion hard-fail logic
- evaluate_trace orchestrates full evaluation pipeline
- All tests pass including existing regression suite
</success_criteria>

<output>
After completion, create `.planning/phases/03-assertion-engine-and-scoring/03-01-SUMMARY.md`
</output>
