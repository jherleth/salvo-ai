---
phase: 03-assertion-engine-and-scoring
plan: 02
type: execute
wave: 2
depends_on: ["03-01"]
files_modified:
  - src/salvo/models/scenario.py
  - src/salvo/loader/validator.py
  - src/salvo/cli/run_cmd.py
  - src/salvo/evaluation/formatting.py
  - tests/test_models_scenario.py
  - tests/test_loader_validator.py
  - tests/test_evaluation_formatting.py
  - tests/test_cli_run.py
autonomous: true
requirements: [EVAL-05, EVAL-06, EVAL-07]

must_haves:
  truths:
    - "User can write operator-key-style assertions in YAML ({path, contains, ...}) and Salvo validates them without extra_forbidden errors"
    - "User runs `salvo run scenario.yaml` and sees per-assertion results with PASS/FAILED/HARD FAIL labels"
    - "User sees minimal output on pass (score + PASS), detailed output on fail (expected vs actual, score breakdown)"
    - "User's run result stored in .salvo/ includes eval_results, correct score, and correct passed status"
    - "Required assertions that fail are labeled HARD FAIL and grouped at top of output"
  artifacts:
    - path: "src/salvo/loader/validator.py"
      provides: "Assertion normalization step in YAML validation pipeline"
      contains: "normalize_assertions"
    - path: "src/salvo/evaluation/formatting.py"
      provides: "Per-assertion result formatting with severity ordering"
      exports: ["format_eval_results"]
    - path: "src/salvo/cli/run_cmd.py"
      provides: "Evaluation pipeline integration replacing Phase 2 stubs"
      contains: "evaluate_trace"
  key_links:
    - from: "src/salvo/loader/validator.py"
      to: "src/salvo/evaluation/normalizer.py"
      via: "normalize_assertions called before Scenario.model_validate()"
      pattern: "normalize_assertions"
    - from: "src/salvo/cli/run_cmd.py"
      to: "src/salvo/evaluation/scorer.py"
      via: "evaluate_trace called after runner.run() returns trace"
      pattern: "evaluate_trace"
    - from: "src/salvo/cli/run_cmd.py"
      to: "src/salvo/evaluation/formatting.py"
      via: "format_eval_results renders per-assertion output"
      pattern: "format_eval_results"
---

<objective>
Wire the assertion engine (from 03-01) into the existing pipeline: update the Assertion model to accept operator-key syntax via pre-validation normalization, integrate evaluate_trace into `salvo run`, and render per-assertion results with severity ordering and score breakdowns.

Purpose: This plan connects the pure evaluation logic from 03-01 to the user-facing pipeline — from YAML loading through execution to CLI output. After this plan, `salvo run` produces real scored results instead of the Phase 2 stub (passed=True, score=0.0).
Output: Working end-to-end evaluation pipeline with formatted output.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-assertion-engine-and-scoring/03-RESEARCH.md
@.planning/phases/03-assertion-engine-and-scoring/03-01-SUMMARY.md
@src/salvo/models/scenario.py
@src/salvo/models/result.py
@src/salvo/loader/validator.py
@src/salvo/cli/run_cmd.py
@src/salvo/execution/trace.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Assertion model update and YAML pipeline normalization</name>
  <files>
    src/salvo/models/scenario.py
    src/salvo/loader/validator.py
    tests/test_models_scenario.py
    tests/test_loader_validator.py
  </files>
  <action>
    **Modify the Assertion model to accept canonical form from normalizer output:**

    The current Assertion model has `extra='forbid'` and fixed fields (type, weight, required, tool, mode, sequence, value, expression, operator). The normalizer (from 03-01) converts shorthand to canonical form with these exact fields, so the model itself does NOT need to change structure — but the validation pipeline must normalize raw assertion dicts BEFORE passing to Pydantic.

    **Update validator.py — insert normalization before Pydantic validation:**

    In `validate_scenario()` function, after receiving `raw_data` dict but before calling `Scenario.model_validate(raw_data)`:
    1. Import `normalize_assertions` from `salvo.evaluation.normalizer`.
    2. If `raw_data` has an `"assertions"` key with a list value, call `normalize_assertions(raw_data["assertions"])` and replace `raw_data["assertions"]` with the result.
    3. Wrap the normalization in a try/except ValueError to catch normalization errors (multiple operators, no operator/type). Convert to ValidationErrorDetail with field="assertions.{index}", type="assertion_normalization_error", and the ValueError message.
    4. This runs before `Scenario.model_validate()` so Pydantic sees canonical fields, not operator keys.

    In `validate_scenario_file()` and `validate_scenario_string()`, no changes needed — they both call `validate_scenario()` which handles normalization.

    **Update Assertion model — add `max_usd` and `max_seconds` fields:**

    The cost_limit and latency_limit assertion types use `max_usd` and `max_seconds` fields respectively. These are not currently on the Assertion model. Add:
    - `max_usd: float | None = None`
    - `max_seconds: float | None = None`

    These fields are only used by their respective evaluators. With `extra='forbid'`, they must be declared on the model to avoid validation errors.

    **Tests:**

    `tests/test_models_scenario.py` — add tests:
    - Test Assertion accepts canonical form (type="jmespath", expression="...", operator="contains", value="hello")
    - Test Assertion accepts cost_limit form (type="cost_limit", max_usd=0.05)
    - Test Assertion accepts latency_limit form (type="latency_limit", max_seconds=10.0)
    - Test Assertion still rejects unknown fields (extra='forbid' still works)

    `tests/test_loader_validator.py` — add tests:
    - Test validate_scenario with operator-key-style assertions succeeds (normalization converts before Pydantic)
    - Test validate_scenario with explicit type assertions passes through
    - Test validate_scenario with invalid assertion (no type, no operator) returns error
    - Test validate_scenario with multiple operator keys returns error
    - Test full Scenario with mixed shorthand and explicit assertions validates correctly
  </action>
  <verify>
    Run `cd /Users/apple/JHERLETH/agent-something && .venv/bin/python -m pytest tests/test_models_scenario.py tests/test_loader_validator.py -v` -- all tests pass.
    Run `cd /Users/apple/JHERLETH/agent-something && .venv/bin/python -m pytest tests/ -v` -- all existing tests still pass (zero regressions).
  </verify>
  <done>
    Operator-key-style assertions in YAML normalize to canonical form before Pydantic validation. Assertion model accepts cost_limit/latency_limit fields. Validation pipeline handles normalization errors gracefully with field-level error reporting. All tests pass.
  </done>
</task>

<task type="auto">
  <name>Task 2: Evaluation integration into run_cmd.py and per-assertion output formatting</name>
  <files>
    src/salvo/cli/run_cmd.py
    src/salvo/evaluation/formatting.py
    tests/test_evaluation_formatting.py
    tests/test_cli_run.py
  </files>
  <action>
    **Create output formatter (src/salvo/evaluation/formatting.py):**

    Per user decisions: "Minimal on pass, detailed on fail, full detail on demand."

    `format_eval_results(eval_results: list[EvalResult], score: float, threshold: float, passed: bool, hard_fail: bool, verbose: bool = False) -> str`:

    Returns a multi-line string for Rich console output.

    **On pass (not verbose):**
    ```
    Score: {score:.2f} (>= {threshold:.2f})  PASS
    ```

    **On pass (verbose):**
    ```
    Score: {score:.2f} (>= {threshold:.2f})  PASS

      {per-assertion listing, all shown}

      Score breakdown:
        {weight breakdown for each assertion}
        ─────────────────────────
        Total: {weighted_sum:.2f} / {total_weight:.2f} = {score:.2f}
    ```

    **On fail:**
    ```
    Score: {score:.2f} (< {threshold:.2f})  FAILED
    ```
    or if hard_fail:
    ```
    Score: {score:.2f}  HARD FAIL (required assertion failed)
    ```

    Then per-assertion details, sorted by severity (hard failures first, then soft failures, then passes):
    - For hard failures: `  HARD FAIL  [required] {assertion description}`
      followed by `             Expected: ...` / `Actual: ...` / `Path: ...`
    - For soft failures: `  FAILED     {assertion description}`
      followed by expected/actual/path details
    - For passes: `  PASS       {assertion description}` (one-line)

    Then score breakdown (always shown on fail):
    ```
      Score breakdown:
        {assertion_desc}    {score} * {weight} = {product}  {[REQUIRED] if required}
        ─────────────────────────
        Total: {weighted_sum:.2f} / {total_weight:.2f} = {score:.2f}
    ```

    Helper `_describe_assertion(result: EvalResult) -> str`: Produce a short description from the assertion type and details. For jmespath: "{path} {operator} {expected_value}". For tool_sequence: "tool_sequence {mode} {sequence}". For cost_limit: "cost_limit max_usd {value}". For latency_limit: "latency_limit max_seconds {value}".

    Note: The EvalResult.details string contains expected/actual/path info from the evaluators. Parse it or use it directly for display. Keep formatting simple — no Rich markup in the formatter, just plain text with indentation. The CLI (run_cmd.py) applies Rich styling.

    **Integrate evaluate_trace into run_cmd.py:**

    Replace the Phase 2 stubs in `_run_async()`:

    1. After `trace = apply_trace_limits(trace)` (step 6), add evaluation step:
       ```python
       from salvo.evaluation import evaluate_trace, normalize_assertions
       ```
    2. Normalize the scenario's assertions: `normalized = normalize_assertions([a.model_dump() for a in scenario.assertions])`. Note: scenario.assertions are already Pydantic Assertion objects (normalized during loading). But evaluate_trace expects dicts. Use `model_dump()` to convert. Actually — since the validator already normalized them and Pydantic validated them, the Assertion objects have canonical fields. Convert with `model_dump(exclude_none=True)` to get clean dicts for evaluators.
    3. Call `eval_results, score, passed = evaluate_trace(trace, normalized, scenario.threshold)`.
    4. Determine `hard_fail` from eval_results: `hard_fail = any(r.required and not r.passed for r in eval_results)`.
    5. Replace the existing summary output section with: first print scenario info (Scenario, Model, Turns, Tools, Tokens, Cost, Latency, Hash), then print evaluation results using `format_eval_results()`.
    6. Update RunMetadata and RunResult to use real `score` and `passed` values (replace the `passed=True, score=0.0` stubs).
    7. Print `[bold green]PASS[/bold green]` or `[bold red]FAILED[/bold red]` or `[bold red]HARD FAIL[/bold red]` using Rich markup.
    8. Exit with code 1 if not passed (for CI compatibility).

    **Tests:**

    `tests/test_evaluation_formatting.py`:
    - Test format_eval_results on pass (not verbose) shows single score line
    - Test format_eval_results on pass (verbose) shows all assertions and breakdown
    - Test format_eval_results on fail shows hard failures first, then soft failures, then passes
    - Test format_eval_results on hard_fail shows "HARD FAIL (required assertion failed)"
    - Test score breakdown math is correct
    - Test _describe_assertion for each assertion type

    `tests/test_cli_run.py` — update existing tests:
    - Update existing CLI run tests to account for real evaluation (no longer stub passed=True). The existing tests use MockAdapter with no assertions in the scenario, so they should still pass vacuously (empty assertions = passed=True, score=1.0).
    - Add test: scenario with assertions that pass -> verify output contains "PASS" and correct score
    - Add test: scenario with assertions that fail -> verify output contains "FAILED" and exit code 1
    - Add test: scenario with required assertion that fails -> verify output contains "HARD FAIL"

    **Update evaluation __init__.py:**
    - Add re-export for `format_eval_results`.
  </action>
  <verify>
    Run `cd /Users/apple/JHERLETH/agent-something && .venv/bin/python -m pytest tests/test_evaluation_formatting.py tests/test_cli_run.py -v` -- all tests pass.
    Run `cd /Users/apple/JHERLETH/agent-something && .venv/bin/python -m pytest tests/ -v` -- all tests pass (zero regressions).
  </verify>
  <done>
    `salvo run` produces real scored results instead of stubs. Per-assertion output shows PASS/FAILED/HARD FAIL labels with severity ordering. Score breakdown shown on fail. Minimal output on pass. RunResult persisted with correct score and passed values. Exit code 1 on failure. All tests pass.
  </done>
</task>

</tasks>

<verification>
- `salvo run` with a scenario containing operator-key assertions produces scored results
- Hard failures (required assertions) labeled distinctly from soft failures
- Score formula: sum(score * weight) / sum(weight) matches computed output
- Empty assertions scenario still passes (vacuous pass)
- Exit code 0 on pass, 1 on fail (CI compatible)
- All tests pass including Phase 1 and Phase 2 regression suite
</verification>

<success_criteria>
- Operator-key-style assertions validate through YAML pipeline without errors
- evaluate_trace replaces Phase 2 stubs in run_cmd.py
- Per-assertion results shown in CLI output with severity ordering
- Score breakdown displayed on fail, minimal on pass
- Required assertion failures produce HARD FAIL label and exit code 1
- RunResult stored with real eval_results, score, and passed values
- All tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/03-assertion-engine-and-scoring/03-02-SUMMARY.md`
</output>
